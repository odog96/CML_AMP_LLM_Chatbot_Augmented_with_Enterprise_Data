The quest for artificial intelligence. Cambridge University Press, 2009. 106 Chris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases, 2022. URL httpstransformercircuits.pub 2022mechinterpessayindex.html. 107 Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das Sarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac HatfieldDodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. Incontext Learning and Induction Heads, September 2022. arXiv2209.11895 cs. URL http arxiv.orgabs2209.11895, doi10.48550arXiv.2209.11895. 108 Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe Global Vectors for Word Representation. In Proceedings of the 2014 Con ference on Empirical Methods in Natural Language Processing EMNLP, pages 15321543, Doha, Qatar, October 2014. Association for Com putational Linguistics. URL httpswww.aclweb.organthology D141162, doi10.3115v1D141162. 109 Mary Phuong and Marcus Hutter. Formal Algorithms for Transformers, July 2022. arXiv2207.09238 cs. URL httparxiv.orgabs2207. 09238. 110 Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking Generalization Beyond Overfitting on Small Algorith mic Datasets. arXiv2201.02177 cs, January 2022. arXiv 2201.02177. URL httparxiv.orgabs2201.02177. 111 Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and Narrowing the Compositionality Gap in Language Models, October 2022. arXiv2210.03350 cs. URL http arxiv.orgabs2210.03350, doi10.48550arXiv.2210.03350. 112 Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pretraining. 2018. Pub lisher OpenAI. 43 113 Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. undefined, 2019. URL httpswww.semanticscholar.orgpaper LanguageModelsareUnsupervisedMultitaskLearnersRadfordWu 9405cc0d6169988371b2755e573cc28650d14dfe. 114 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Explor ing the Limits of Transfer Learning with a Unified TexttoText Trans former. arXiv1910.10683 cs, stat, July 2020. arXiv 1910.10683. URL httparxiv.orgabs1910.10683. 115 Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield Networks is All You Need, April 2021. arXiv2008.02217 cs, stat. URL httparxiv.orgabs2008.02217, doi10.48550arXiv. 2008.02217. 116 Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory. arXiv2106.10165 hepth, stat, August 2021. arXiv 2106.10165. URL httparxiv.orgabs2106.10165. 117 Frank Rosenblatt. The perceptron a probabilistic model for information storage and organization in the brain. Psychological review, 656386, 1958. 118 David E. Rumelhart, Geoffrey E. Hinton, and James L. McClelland. A general framework for parallel distributed processing. 1986. 119 Stuart J Russell. Artificial intelligence a modern approach. Pearson Ed ucation, Inc., 2010. 120 Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. Are emergent abilities of large language models a mirage? ArXiv, abs2304.15004, 2023. 121 Terrence J Sejnowski. The deep learning revolution. MIT press, 2018. 122 Claude E Shannon. Xxii. programming a computer for playing chess. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 41314256275, 1950. 123 Hava T Siegelmann and Eduardo D Sontag.