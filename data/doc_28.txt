One can thus hypothesize that LLMs have commonalities with the human language module and might be useful scientific models for it,41 but that progress towards human level capability will eventually stall without analogs of the other modules. 73 A related claim is that current LLMs, even when they perform well on bench marks, do not construct models of the world. Consider reasoning about spatial relations for example if A is in front of B is in front of C, then A is in front of C. Such reasoning is greatly facilitated by representing the locations of objects in space, perhaps in terms of coordinates, perhaps using place cells or in some other nonlinguistic way. If distance from the observer is explicitly represented and used in reasoning, then it becomes hard to get this type of question wrong. Conversely, to the extent that LLMs do get it wrong, this might be evidence that they lack this type of world model or cannot effectively use it. There are many papers exhibiting LLM errors and suggesting such inter pretations, but often one finds that next years model does not make the same errors. At the present rate of progress it seems premature to draw any strong conclusions. My own opinion is that there is no barrier in principle to LLMs constructing internal nonlinguistic models of the world, and the work 78 on OthelloGPT discussed in 7 is a nice demonstration of what is possible. This is not to say that any and all models can be learned, but rather that it might be better for now to focus on other significant differences between LLM and human reasoning, of which there are many. I will come back to this below. If LLMs and other connectionist systems do not work in the same way as brains, what other guidance do we have? In 7 we discussed one answer, the hypothesis that they work much like the algorithms and circuits studied in 41Chomsky rejects this idea, saying that The childs operating system is completely differ ent from that of a machine learning program. New York Times, March 8, 2023. 27 computer science. Perhaps trained LLMs implement algorithms like those de signed by computational linguists, or perhaps new algorithms which were not previously thought up but which can be understood in similar terms. In either version this is still a hypothesis, but if we grant it we can draw on insights from theoretical computer science which apply to all such algorithms. Computational complexity theory 4, 137 makes many statements and con jectures about how the time and space required by a particular computation depends on the size of the problem usually meaning the length of the input. The most famous of these, the P NP conjecture, states very loosely that for problems which involve satisfying general logical statements, finding a solution can be much harder than checking that the solution is correct.