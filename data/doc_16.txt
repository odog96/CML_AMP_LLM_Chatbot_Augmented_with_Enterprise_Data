Using todays FFNs, one could implement this with L 100 or so.There does not seem to be much work on large fully connected FFN languagemodels, because by the time the technology advanced to this point the far moreefficient transformer models had taken over. Still, they illustrate the general29Here w denotes the number of occurences of w in the corpus. These ratios can alsobe expressed in terms of the pairwise mutual information, PN w, uP wP u.17idea and also one of its most obvious limitations. Even with L 100, oftenpredicting the next word requires remembering words which appeared fartherback. To solve this problem we need to incorporate some sort of memory intothe model.The simplest memory is an additional state variable which is updated witheach word and used like the other inputs. To do this, we should take the stateto be a vector in some Rq. This brings us to the recurrent neural network orRNN. Its definition is hardly any more complex than what we saw before. Witheach word position say with index i we will associate a state vector si whichcan depend on words up to wi and on the immediately previous state. Then,we let the map F determine both the next word and the next state asvi1, si1 F si, vi, vi1, vi2, . . . , vik1,12where the parenthesis notation on the left hand side means that the outputvector of F is the concatenation of two direct summand output vectors.Mathematically, Eq. 12 is a discrete dynamical system. If we grant thatF can be an arbitrary map, this is a very general class of systems. One wayof characterizing its generality is through computational complexity theory, byasking what classes of computation it can perform. In 123 it was argued thatthe RNN is a universal computer, but this granted that the computation of Fin Eq. 11 could use infinite precision numbers. Under realistic assumptionsthe right complexity class is a finite state machine, which can recognize regularlanguages 26, 134. We will say more from this point of view in 7.There are many variations on the RNN such as LSTMs 57, each with theirown advantages, but we must move on.6 Recipe for an LLMWe are now ready to define the transformer model.30 It is simply another classof maps F from lists of vectors to a vector to be used in the prescription above.Indeed, it is a natural generalization of the FFN which is associated to permutational symmetry. This is in direct analogy to the use of convolutional neuralnetworks CNNs for image recognition, which are FFNs which are equivariantunder the symmetry of translations in two dimensions which is natural for theset of images.A transformer is a composition of two types of functions layers taken inalternation, each mapping an input list of L vectors ui to an output list of Lvectors vi.