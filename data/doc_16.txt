Given a corpus, we define its N gram cooccurrence matrix MN to be the W W matrix whose w, w entry counts the number of N grams in the corpus containing both words. This matrix defines a map from words to vectors W Rp 7 where the dimension p W, by taking a word to the corresponding column of MN . Such a map is called a word embedding. Applying this map to each word independently, we can map a string of k words in W k to a string of vectors, and this is the next step after tokenization of LLM processing. One might worry that these are very high dimensional vectors with many zero entries, which seems wasteful. A standard statistical cure for this problem is to do principal component analysis PCA. In words, instead of columns of MN we use the columns of a p W matrix Z chosen such that Z tZ is the best rank p approximation to MN in the sense that it minimizes tr Z tZ MN 2. One can do better, but this gives the right idea. Next, we feed this string of vectors into some machine learning model to get an output which we use to predict the next word. If we just want the most likely next word, a good way is to output a vector v Rp, and choose the word w which maximizes the inner product v w. We denote this relationship as v w. More generally, the standard inverse map from a vector to a probability distribution on words is the Boltzmann distribution on the inner products. Explicitly, we postulate an inverse temperature 1T and take28 v P w evw w evw cid80 8 Here is an observation 99 which supports the idea that word embeddings contain information about meaning. Since the embeddings are vectors, they can be added. Consider the following equation king man woman ? 9 28T is the temperature parameter which can be set in say the GPT user interface. Also, this ratio of exponentials is usually called softmax in machine learning as its limit is the argmax function producing a vector whose nonzero components have the same index values as the largest of the inputs. 16 One might hope that the word which maximizes this inner product is queen, and indeed it is so. There are many more such examples empirically one needs the dimension p 100 for this to work. One can argue 108, 5 that it follows from relations between cooccurence statistics29 w, MN w, kingking MN w, queenqueen MN w, manman MN w, womanwoman 10 Given these ideas and a map F from a list of vectors to a vector, we can now propose a very general class of Lgram autoregressive language models as the combination of the following steps 1. Map the L input words wi to L vectors wi. 2. Apply F to the list of these vectors to get a prediction vector v. 3.