On the computational power of neural nets. In Proceedings of the fifth annual workshop on Computa tional learning theory, pages 440449, 1992. 124 Brian Cantwell Smith. Procedural reflection in programming languages volume i. 1982. 44 125 Jascha SohlDickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynam ics. In International Conference on Machine Learning, pages 22562265. PMLR, 2015. arXiv1503.03585. 126 Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws beating power law scaling via data pruning, June 2022. Number arXiv2206.14486 arXiv2206.14486 cs, stat. URL httparxiv.orgabs2206.14486, doi10.48550 arXiv.2206.14486. 127 Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the Imitation Game Quantifying and extrapolating the capabilities of lan guage models. Technical Report arXiv2206.04615, arXiv, June 2022. arXiv2206.04615 cs, stat type article. URL httparxiv.orgabs 2206.04615. 128 Richard Sutton. The bitter incompleteideas.netIncIdeasBitterLesson.html. lesson, 2019. URL httpwww. 129 Christian Szegedy. A promising path towards autoformalization and gen eral artificial intelligence. In International Conference on Intelligent Com puter Mathematics, 2020. 130 Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gim pel. Chess as a Testbed for Language Model State Tracking, May 2022. arXiv2102.13249 cs. URL httparxiv.orgabs2102.13249, doi10.48550arXiv.2102.13249. 131 Richard E. Turner. An Introduction to Transformers, July 2023. arXiv2304.10557 cs. URL httparxiv.orgabs2304.10557, doi 10.48550arXiv.2304.10557. 132 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Atten tion Is All You Need. June 2017. arXiv 1706.03762. URL https arxiv.orgabs1706.03762. 133 Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas tian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent Abilities of Large Language Models. 2022. Publisher arXiv Version Number 2. URL https arxiv.orgabs2206.07682, doi10.48550ARXIV.2206.07682. 134 Gail Weiss, Yoav Goldberg, and Eran Yahav. On the Practical Compu tational Power of Finite Precision RNNs for Language Recognition, May 2018. arXiv1805.04908 cs, stat. URL httparxiv.orgabs1805. 04908, doi10.48550arXiv.1805.04908. 45 135 Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. NaturalProofs Mathematical Theorem Prov ing in Natural Language. page 14, 2021. 136 Noam Wies, Yoav Levine, and Amnon Shashua. The Learnability of In Context Learning, March 2023. arXiv2303.07895 cs. URL http arxiv.orgabs2303.07895, doi10.48550arXiv.2303.07895. 137 Avi Wigderson. Mathematics and computation A theory revolutionizing technology and science. Princeton University Press, 2019. 138 Wikipedia. URL httpsen.wikipedia.orgwikiReflective programming. 139 Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An Explanation of Incontext Learning as Implicit Bayesian Inference, July 2022. arXiv2111.02080 cs. URL httparxiv.orgabs2111.02080, doi10.48550arXiv.2111.02080. 140 Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jian feng Gao. Tensor Programs V Tuning Large Neural Networks via Zero arXiv2203.03466 cond Shot Hyperparameter Transfer, March 2022. mat. URL httparxiv.orgabs2203.03466, doi10.48550arXiv. 2203.03466. 141 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts Deliberate Prob lem Solving with Large Language Models, May 2023. arXiv2305.10601 cs. URL httparxiv.orgabs2305.10601, doi10.48550arXiv. 2305.10601.