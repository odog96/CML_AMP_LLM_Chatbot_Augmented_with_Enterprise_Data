Suppose the product u B v is the dot product, then attention selects the input components uj most similar to the current units input, uj ui in the notation of 5. The matrix B allows for comparing different parts of the embeddings and ignoring other parts, in a way determined by optimizing the objective function Eq. 3. Composing these two types of functions or layers produces a map from RpL to RpL. Often one takes, instead of the pure FFN and attention func tions, sums of these with the identity function residual connections. The FFNs generally have a single hidden layer which can be of a different dimension, call this ph.32 Finally, while the language model prescription asked for a map to Rp, this is easily obtained by just taking the last vector in the final output list. There are two more essential details to cover and many minor details we will skip. The first is the concept of attention head. The definition Eq. 13 allowed for a general linear transformation W whose range is the output vector. We are free to choose its dimension, call it q, and typically one takes this to In return one can use many be much less than the embedding dimension p. copies of Eqs. 13,14 in parallel with different choices for B and W , to produce many outputs. One can then concatenate these outputs to get a final output of dimension p. These copies are called attention heads and we will denote their number by H, so p Hq. The second essential detail is that, so far, there is nothing in the definition that keeps track of the order of the list of input vectors the output of Eq. 13 will be invariant under a general permutation of the input vectors. While this is an elegant property, it is not what we want for processing language, for which the order of the words matters. The cure for this is very simple one takes as inputs not the word embeddings Eq. 7, but the direct sum concatenation of these with positional embedding vectors, i.e. vectors which encode the position index of the word in the string. These can be a combination of sines and 31The restriction i j to previous or current inputs is done to get an autoregressive model one can relax this for other purposes. 32Explicitly, vi W1 max0, W0 ui b0 b1, where b0,1 are more learnable parameters. 19 cosines of various frequencies, such as 132 e2i1, e2i cos position 100002idpos , sin position 100002idpos i 1, . . . , dpos 2 15 One could instead treat these vectors as learnable parameters. Still, the trig function basis for positions may be significant. It has been generalized to rep resent other graph structures by using eigenfunctions of the graph Laplacian as positional embeddings. The invariance of the transformer model under permutation symmetry is reminiscent of the point we mentioned earlier, that translation symmetry mo tivates the CNN.