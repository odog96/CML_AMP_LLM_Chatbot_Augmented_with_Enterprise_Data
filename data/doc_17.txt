Use the inverse map Eq. 8 to get a probability distribution over words. Furthermore, if the map F has parameters, given a corpus we can determine them by optimizing the function Eq. 3 with respect to the parameters. And once we bring in optimization, we can also optimize with respect to the coefficients of the embedding map Eq. 7, so that we can dispense with cooccurence statistics. This is the general prescription followed by the LLMs, and to complete it we just need to specify a family of maps F . One possibility is to use a general fully connected feed forward neural network FFN, also called MLP for multilayer perceptron. We recall that an FFN is a composition of two general types of functions, linear maps Wi and nonlinear maps , so that F v Wd Wd1 . . . W1 W0. 11 In more concrete terms, the maps Wi are multiplication by rectangular matrices of parameters usually called weights in this context, while the maps act independently on each component of their input vector by a fixed nonlinear function such as tanh or more typically ReLU identity for x 0 and zero for x 0. The main fact we recall about FFNs is that, in the limit that the number of parameters becomes large, they can approximate any given function arbitrarily well 38. We refer the reader interested in learning more to 40, 116. We can get a very natural deep learning version of the Lgram models by using an FFN for the map F in the prescription above 18. Since this asked for a map from a list of vectors to a vector, we need to convert the input list into a single vector. This is easy we can take the direct sum of the input vectors, i.e. the dimension L p vector whose components are the concatenated lists of their components. Using todays FFNs, one could implement this with L 100 or so. There does not seem to be much work on large fully connected FFN language models, because by the time the technology advanced to this point the far more efficient transformer models had taken over. Still, they illustrate the general 29Here w denotes the number of occurences of w in the corpus. These ratios can also be expressed in terms of the pairwise mutual information, PN w, uP wP u. 17 idea and also one of its most obvious limitations. Even with L 100, often predicting the next word requires remembering words which appeared farther back. To solve this problem we need to incorporate some sort of memory into the model. The simplest memory is an additional state variable which is updated with each word and used like the other inputs. To do this, we should take the state to be a vector in some Rq. This brings us to the recurrent neural network or RNN. Its definition is hardly any more complex than what we saw before.