Conversely, one might conjecturethat any task for which there is an algorithm in this class can be learned, atleast in the limit of an infinite amount of training data.What about the lenses of pure mathematics, theoretical physics and alliedfields? Besides my own personal interest in them, these fields have made substantial contributions to statistics and machine learning, especially the interfacebetween statistical physics and machine learning is a vibrant field of research71, 97. Spin glass theory made a very deep impact, starting with the Hopfieldmodel and developing into a farreaching theory of optimization landscapes andcomplexity. Random matrix theory is central to high dimensional statistics 63and in many approaches to understanding deep learning 116. Mathematicalapproaches to language such as 20, 34, 86, 89 can reveal new structure andprovide deeper understanding.Another reason to think pure mathematics and theoretical physics have moreto contribute is that neural networks, transformers, and many of the models ofneuroscience, are formulated in terms of real variables and continuous mathematics. By contrast, computer science is largely based on discrete mathematics,appropriate for some but not all questions. Perhaps word embeddings have important geometric properties, or perhaps the dynamics of gradient descent arebest understood through the intuitions of continuous mathematics and physics.Arguments such as those in 7 which reduce neural networks to digital circuits,even if they do explain their functioning, may not be adequate to explain howthey are learned.Having at least mentioned some of the many points of view, let me combinethese insights and speculate a bit on where this is going. Let me focus onthree capabilities which seem lacking in current LLMs planning, confidence28judgments, and reflection.Planning, solving problems whose solution requires choosing a series of actions andor the consideration of future actions by other agents, is one of thecore problems of AI. Making plans generally requires search, and in generalsearch is hard assuming P NP. A familiar example is a chess program,which searches through a game tree to judge the longer term value of a candidate move by hypothesizing possible future moves. While much of the successof AlphaGo and AlphaZero is attributed to reinforcement learning by selfplay,they also search through game trees indeed the Monte Carlo tree search algorithm on which they built 36, 23 was considered a key enabling breakthrough.By contrast, LLMs have no component dedicated to search. While it doesnot seem impossible that search trees or other structures could be learned internally like world models, it seems intuitively clear that an autoregressive modelwhich predicts one word at a time and cannot go back to revise its predictionsin light of what comes later will be seriously handicapped in planning. Thisobservation is motivating a fair amount of current work on ways to incorporatesearch. LeCun has suggested adding a dynamic programming component tosearch through multiword predictions, as part of his path towards autonomousmachine intelligence 75. Another proposal, the tree of thoughts model 141,works with a search tree of LLM responses.