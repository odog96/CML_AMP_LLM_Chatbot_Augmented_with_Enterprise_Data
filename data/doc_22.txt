One familiar starting point in neuroscience is to measure the activity of neurons and try to correlate it with properties of the system inputs or outputs. The grandmother cell which fires when a subject sees his or her grandmother is an extreme and controversial example. Better established are the place cells in the hippocampus which fire when an animal passes through a specific part of its environment. Generally there is no reason why the representation should be so direct there might be some neural code which maps stimuli onto specific combinations or patterns of activity. The details of the neural code could even be different between one individual and the next. Analogous concepts in LLMs are the maps from input strings to intermediate results or activations. The first of these is the embedding map Eq. 7. Considering each layer in succession, its outputs sometimes called contextualized embeddings also define such a map. The details of these maps depend on details of the model, the training dataset and the choices made in the training procedure. Besides the hyperparameters, these include the random initializations of the parameters, the order in which data items are considered in training and their grouping into batches. Even small differences can be amplified by the nonlinear nature of the loss landscape. One way to deal with this indeterminacy is to look for structure in the maps which does not depend on these choices. The linear relations Eq. 9 between word embeddings are a very elegant example, telling us and presumably the model something about the meanings of the words they represent. Moving on to the later layers, one can ask whether contextualized embeddings carry information about the grammatical role of a word, about other words it is associated to such as the referent of a pronoun, etc.. One can go on to ask whether any of the many structures which one would think need to be represented to understand the real world, are visible in these embeddings. Many structures are too intricate to show up in linear relations. A more general approach is to postulate a target for each training data item and train a probe model usually an FFN to predict it from the embeddings. If this works, one can go on to modify the internal representation in a minimal way which changes the probe prediction, and check if this leads to the corresponding effects on the output see 12 and references there. This procedure is simpler to explain in an example. A pretty example of probing for a world model is the recent work of Li et al 78 see also 130 on representations in a transformer model trained to play the board game Othello.37 37For readers not familiar with this game, two players alternate in placing black and white 22 They train a model OthelloGPT 38 to take as input a sequence of 60 legal moves, for example E3 D3 ... in the standard algebraic notation, and at each step to predict the next move.