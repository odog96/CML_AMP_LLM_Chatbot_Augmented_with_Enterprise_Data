3202luJ11LC.sc1v28750.7032viXraLarge Language ModelsMichael R. DouglasCMSA, Harvard UniversityDept. of Physics, Stony Brook Universitymdouglascmsa.fas.harvard.eduJuly 2023AbstractArtificial intelligence is making spectacular progress, and one of thebest examples is the development of large language models LLMs suchas OpenAIs GPT series.In these lectures, written for readers with abackground in mathematics or physics, we give a brief history and surveyof the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs workand how models trained to predict the next word in a text are able toperform other tasks displaying intelligence.1 1IntroductionAt the end of November 2022, OpenAI released a system called ChatGPT whichinteracts with its users in natural language. It can answer questions, engage indialogs, translate between languages and write computer code with a fluencyand ability far exceeding all previous publically available systems. Although itfalls well short of human abilities in many ways, still the large language modeltechnology of which it is an example is widely considered to be a major advancein artificial intelligence.1Few developments in science and technology entered the popular consciousness as quickly as ChatGPT. There is no mystery about why. The ability to uselanguage is a defining property of humanity, and for the first time a computeris doing this well enough to make a comparison with humans interesting. Allof the hopes and fears which have developed around AI, robots and technologymore generally are being brought into the discussion.In my opinion this isjustified the speed of recent progress makes it urgent to better understand AI,to forecast its capabilities and limitations, and to make wise decisions aboutits development and use. With great opportunities will come great challenges,which will concern all of us.In these lecture notes we give an introduction to this subject for mathematicians, physicists, and other scientists and readers who are mathematicallyknowledgeable but not necessarily expert in machine learning or artificial intelligence. We begin with a very brief overview of AI in 2 to explain some ideaswe consider to be essential context, the basic principles of the symbolic andconnectionist approaches. In 3 we define statistical language models and relatethe history of transformerbased LLMs up through GPT4. In 4 we discussmeasures of what LLMs do and how well they do it. We then give a preciseexplanation of simpler language models in 5 and the transformer architecturein 6.It is amazing that a model defined by a few short equations, trained to gothrough a text and simply predict each next word as it appears a task whichseems only loosely related to any definition of intelligence can do tasks whichobviously require intelligence, such as solving word problems like the one inFigure 1 below. At present nobody really understands how this works. Eventhe interpretation of what LLMs are doing is controversial, ranging from thebelief that they are simply rearranging the sentences they were trained on,all the way to the belief that the LLMs are learning sophisticated models ofthe world and that simply scaling up the computations will produce artificialgeneral intelligence.