While much of the success of AlphaGo and AlphaZero is attributed to reinforcement learning by selfplay, they also search through game trees indeed the Monte Carlo tree search algo rithm on which they built 36, 23 was considered a key enabling breakthrough. By contrast, LLMs have no component dedicated to search. While it does not seem impossible that search trees or other structures could be learned inter nally like world models, it seems intuitively clear that an autoregressive model which predicts one word at a time and cannot go back to revise its predictions in light of what comes later will be seriously handicapped in planning. This observation is motivating a fair amount of current work on ways to incorporate search. LeCun has suggested adding a dynamic programming component to search through multiword predictions, as part of his path towards autonomous machine intelligence 75. Another proposal, the tree of thoughts model 141, works with a search tree of LLM responses. A system which uses hierarchical planning for mathematical theorem proving was developed in 62. The next capability on my list, making and working with confidence judg ments, has to do with the well known hallucination problem, that LLMs often simply invent statements, including untrue facts and imaginary citations. While advantageous for a poetry generator, and bearable for a system which makes suggestions which an expert human user will verify, this is a huge obstacle to many practical applications. Thus it is the subject of a great deal of research a few of this months papers are 43, 79, 81. Perhaps by the time you read these words there will have already been major progress. Why are LLMs producing these hallucinations? One intuition is that they are doing some sort of compression, analogous to JPEG image compression, which introduces errors 29. This point of view suggests that the problem will eventually be solved with larger models and perhaps better training protocols which focus on the more informative data items 126. A related intuition is that the problems follow from inability to properly generalize. This comes back to the point about world models a correct model, for example an internal encoding of place information, by definition correctly treats the properties being modeled. Now suppose we grant that the LLM is solving some class of problems, not by constructing such a model, but by rulebased reasoning. In other words, the LLM somehow learns rules from the corpus which it uses to make particular inferences which agree with the model. While such rules can cover any number of cases, there is no clear reason for such a rule set to ever cover all cases. Another intuition is that the training data contains errors and this is reflected in the results. Certainly the internet is not known for being a completely reliable source of truth. This intuition also fits with the observation that adding code 29 computer programs to the training set improves natural language reasoning.