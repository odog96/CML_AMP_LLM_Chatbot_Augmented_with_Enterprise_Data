Building a large annotated corpus of english The penn treebank. 1993.91 David Marr. Vision A computational investigation into the human representation and processing of visual information. MIT press, 2010.92 Jiri Matousek. Lecture notes on metric embeddings. 2013. URL httpskam.mff.cuni.czmatousekbaa4.pdf.93 Pamela McCorduck and Cli Cfe. Machines who think A personal inquiryinto the history and prospects of artificial intelligence. CRC Press, 2004.94 William Merrill. On the Linguistic Capacity of RealTime Counter Automata. arXiv2004.06866 cs, April 2020. arXiv 2004.06866. URLhttparxiv.orgabs2004.06866.95 William Merrill and Ashish Sabharwal. The Parallelism Tradeoff Limitations of LogPrecision Transformers, April 2023.arXiv2207.00729cs. URL httparxiv.orgabs2207.00729, doi10.48550arXiv.2207.00729.96 William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated Transformers are ConstantDepth Threshold Circuits. arXiv2106.16213 cs,April 2022. arXiv 2106.16213. URL httparxiv.orgabs2106.16213.97 Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.98 Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The Quantization Model of Neural Scaling, March 2023. arXiv2303.13506 condmat. URL httparxiv.orgabs2303.13506, doi10.48550arXiv.2303.13506.99 Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. EfficientEstimation of Word Representations in Vector Space, September 2013.arXiv1301.3781 cs. URL httparxiv.orgabs1301.3781.100 Marvin Minsky. Society of mind. Simon and Schuster, 1988.101 David Mumford. Pattern theory the mathematics of perception. arXivpreprint math0212400, 2002.102 David Mumford and Agnes Desolneux. Pattern theorythe stochasticanalysis of realworld signals. CRC Press, 2010.42103 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability,January 2023.arXiv2301.05217 cs. URL httparxiv.orgabs2301.05217, doi10.48550arXiv.2301.05217.104 Allen Newell, John Clifford Shaw, and Herbert A Simon. Empirical explorations of the logic theory machine a case study in heuristic. In Paperspresented at the February 2628, 1957, western joint computer conferenceTechniques for reliability, pages 218230, 1957.105 Nils J Nilsson. The quest for artificial intelligence. Cambridge UniversityPress, 2009.106 Chris Olah. Mechanistic interpretability, variables, and the importance ofinterpretable bases, 2022. URL httpstransformercircuits.pub2022mechinterpessayindex.html.107 Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, AnnaChen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac HatfieldDodds,Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, LianeLovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, JaredKaplan, Sam McCandlish, and Chris Olah.Incontext Learning andInduction Heads, September 2022. arXiv2209.11895 cs. URL httparxiv.orgabs2209.11895, doi10.48550arXiv.2209.11895.108 Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVeGlobal Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing EMNLP,pages 15321543, Doha, Qatar, October 2014. Association for Computational Linguistics. URL httpswww.aclweb.organthologyD141162, doi10.3115v1D141162.109 Mary Phuong and Marcus Hutter. Formal Algorithms for Transformers,July 2022. arXiv2207.09238 cs. URL httparxiv.orgabs2207.09238.110 Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and VedantMisra. Grokking Generalization Beyond Overfitting on Small Algorithmic Datasets. arXiv2201.02177 cs, January 2022. arXiv 2201.02177.URL httparxiv.orgabs2201.02177.111 Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith,and Mike Lewis. Measuring and Narrowing the Compositionality Gapin Language Models, October 2022. arXiv2210.03350 cs. URL httparxiv.orgabs2210.03350, doi10.48550arXiv.2210.03350.112 Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.Improving language understanding by generative pretraining. 2018. Publisher OpenAI.43113 Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, andIlya Sutskever. Language Models are Unsupervised Multitask Learners.undefined, 2019.