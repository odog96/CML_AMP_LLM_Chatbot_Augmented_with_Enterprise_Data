Another is theEleutherAI Language Model Evaluation Harness21 and leaderboard.22 Thebenchmark suite HELM 80 measures additional metrics such as tendency torepeat copyrighted material, bias, toxicity and the like.18We are calling this phenomenology following the physics use of the term, not its use inpsychology and philosophy to describe the study of subjective experience.19A potential pitfall is that after a benchmark is published, the test items can find theirway into future training data and then be solved by memorization. Methods to detect andprevent this are discussed in the references.20httpspaperswithcode.comdatasetbigbench21httpsgithub.comEleutherAIlmevaluationharness22httpshuggingface.cospacesHuggingFaceH4open llm leaderboard12Reasoning ability is of particular interest for mathematical and scientific applications of course we all look forward to the day when computers will help usgrade assignments, referee papers and do our research. There are many benchmarks for solving logical problems expressed in natural language. Benchmarksfor mathematical theorem proving include NaturalProofs 135, MiniF2F 144and ProofNet 6 as of mid2023 LLMs and the best other systems can findmany proofs 2080 but still fail on some seemingly easy cases. Simpler aspects of reasoning which have benchmarks are the ability to deal with negation142, consistency between different phrasings of the same question 61, andcompositionality the ability to analyze statements and problems into simplerparts, solve these and combine the results 111.Natural language tasks are very complex, and benchmarks constructed fromreal world data cannot be used directly in theoretical considerations. For thispurpose one generally defines toy worlds and generates synthetic data. Thepossibilities are endless, but some which have been used are arithmetic problems decimal arithmetic modular arithmetic, game play, solving systems ofequations, and parsing formal languages. A particularly interesting task is linear regression 48 since this is the prototypical case of statistical inference, asystem which learns to do it can be said to be learning how to learn.Coming to scaling laws, denote the model size number of parameters asP and the dataset size number of tokens in the corpus as D, then there aretwo general regimes. If we hold one of these say P fixed and take the othersay D to infinity, then a law of large numbers applies and L 1D. On theother hand, if we take one parameter very large and study the dependence onthe other, nontrivial power law scaling can emerge.In principle one can getdifferent exponents for D and P , suggesting the ansatzLP, D cid19P Dcid34cid18 PcPcid35D.DcD4where L is test loss Eq. 3 computed in an optimally regularized model.23 Thisis a good fit to Figure 2.While in Figure 2 the two exponents appear to differ, there is not reallyconvincing evidence that this is significant. Before working hard on this, oneshould ask if there is any way to control the many choices involved, so as to defineuniversal exponents. One context in which this can be studied systematically istransfer learning, by distinguishing the dependence on the pretraining and finetuning datasets 55. Another relevant and practical question is whether onecan prune the dataset to improve the scaling.