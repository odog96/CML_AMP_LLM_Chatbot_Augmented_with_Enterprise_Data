While intuition based on human abilities might find this unremarkable, it is actually quite unusual for an ML model and this is why the pretrainingfine tuning paradigm was the usual approach in previous work. Of course the train ing set already contains many examples of QA pairs. More striking are tasks which are not much represented in the training set, such as finding anagrams or rearranging letters in words. One can even do incontext metalearning of machine learning tasks such as linear regression see 4. Once it is established that the model can generalize from a few examples, a further step towards human capabilities is to try zero examples, instead simply explaining the task in natural language. At this point it becomes difficult to classify the tasks should we consider the task of writing code from a natural language specification to be a form of translation, or an example of explaining the task, or something else? The relation between the input text or prompt 17A quantitative version of this claim is that performance for the emergent capability improves rapidly at some threshold value of the word prediction loss. This claim is disputed, see 120, 133 for discussion. 11 Dataset Size tokensParameters nonembeddingCompute PFdays, nonembeddingTest Loss and the output has many surprising features. For example, a standard tech nique in LLM question answering which measurably improves performance is to precede the question with a prompt such as I will answer this question help fully and truthfully. Is this somehow biasing the network towards certain texts and away from others after all the internet corpus is hardly a reliable source of truth ? Suppose we have a theory of how this works, how can we test it? Does the model know anything about the truth of statements? 25, 79 As has been much reported, one of the major difficulties in using LLMs for practical tasks is their propensity to invent facts especially citations and their limited ability to do logical reasoning, algebra and other symbolic tasks. A device for improving this, called chain of thought prompting, is to give examples say of question answer task for definiteness with some intermediate reasoning steps spelled out. This was used in the Minerva QA system 77 which produced the example in Figure 1. Still the fraction of problems it solved correctly is around 50 the later GPT4 is similar. Even for simpler questions, the reliability of GPT4 is more like 90. Much current research is devoted to this problem of reliable reasoning, as we discuss in 8. 4 Phenomenology of language models In this section we discuss general claims, noninvasive experiments, and the oretical arguments which do not depend on microscopic details of the models such as the trained weights.18 This includes evaluation of model capabiliities, qualitative observations and scaling laws. What can LLMs do?