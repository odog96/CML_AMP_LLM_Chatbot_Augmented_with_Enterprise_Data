Following the general machinelearning experience that supervised tasks learning from inputoutput pairsare easier than unsupervised tasks, many of these works addressed machinetranslation and parsing, for which there are good labeled datasets documentswith their translations sentences with their grammatical structure. Howeverunlabeled datasets are much larger and by 2015 or so there was a sense that selfsupervised learning was the next frontier 74, leading to more focus on maskedword prediction.The history of transformer models starts with the 2017 proposal of Vaswaniet al 132. Their model was designed for a translation task and was more complicated than what we will explain in 6, but the essential idea to use attentionand positional encoding to represent all the relations between the words in atext originated here and is fully present.The transformer architecture was taken up by many groups, and particularlyinfluential 2018 works include BERT 39 and GPT 112. BERT was trained bymasking arbitrary words in a sentence not just the next word, which allowsthe model to look backward and forward for context and leads to better results.However it is not straightforward to sample from such a model, and eventuallythe simpler next word prediction approach followed by GPT won out.Both of these models, and most work of this period, followed the paradigmof pretraining followed by fine tuning. The idea was to first train for wordprediction on a very large corpus, to get a general purpose model. This wouldthen be adapted to specific tasks such as question answering by fine tuning.This means doing a second pass of supervised learning on a much smaller labeled12With the sign, L 0 and better models have smaller L. The term loss function is oftenused for an objective function with these properties.13In CS this term generally refers to the large scale arrangement of components of a system.9dataset, replacing next word prediction by the objective function for the specifictask. Say we are doing question answering, this could be the accuracy of theanswers. This two step procedure was justified by the notion of transfer learning,meaning that the capabilities of the general purpose model transfer to relatedbut different tasks. This approach led to SOTA14 results on many benchmarksand motivated much further work.Most importantly, a great deal of ingenuity and hard work was put intosolving the engineering problems of training larger and larger models on largerand larger datasets. As for the data, a lot of text is available on the web, withone much used archive of this data provided by Common Crawl.15 Training canlargely be done in parallel by dividing up this data, and the availability of largeclusters of GPUenabled servers at industrial labs and through cloud computingmeant that sufficient computing resources were available in principle. However,the overall cost of training scales as at least the product of model size anddataset size, and this was becoming expensive. While the precise cost figuresfor the GPT series are not public, it is estimated that a single training runof the largest GPT3 models cost tens of millions of dollars.