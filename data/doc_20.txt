However permutation symmetry is badly broken in language, even in the simplest formal languages,33 and it is not obvious why this should be a useful property for the model to have. One might argue that although any particular language breaks permutation symmetry, it acts naturally on the en semble of languages and thus should have a simple representation. For example, besides the usual infix arithmetic notation a b, one could instead use prefix a b or postfix a b . Translating between these notations is arguably easier for permutation invariant maps using position embeddings. An oppos ing view would be that permutation symmetry is just a secondary property of the simplest model using attention, and that the main point is to explain the value of attention. In addition to its ability to select similar items, it provides a simple way to take products of embedding vectors. In computational com plexity terms, attention enlarges the class of circuits which can be simulated by a constant depth transformer 28, 41, 95, 96. Physics analogies of Eqs. 13,14, especially to the Hopfield model, may be important 59, 115. A major practical advantage of the transformer over the RNN and other previous architectures is that the computations in the attention mechanism can be done in parallel, so given sufficiently many processors the time required does not increase with the window length L. This is by contrast with the RNN in which information propagates from one word to the next, so a window of length L requires time L to process. On the other hand the ability of each unit to pay attention to every previous unit means that the total computation required by the transformer scales as L2. This is the limiting factor for increasing L and this is widely seen as a problem. There has been a lot of work to improve this scaling, by removing some of the connections as in sparse attention 30, by introducing multiscale structure, or in other ways. Let us summarize by listing the hyperparameters34 and their values for the largest 175B GPT3 22. They are Embedding dimension p 12288 and hidden layer dimension ph 4p. Window length L 4096 or 8192. Depth D 96, counting both FFN Eq. 11 and attention Eq. 13 layers.35 33Compare the logical implications A B and B A. 34This term refers to model choices which are not learned through gradient descent. 35Some of the attention layers in GPT3 are sparse. 20 Number of heads H 96 the equality with D is a coincidence as far as I know. The total number of parameters is roughly 12Dp2. As mentioned earlier, all of these parameters, and the parameters of the em bedding map Eq. 7, are determined as follows. One generally starts with ran dom initial conditions, usually meaning that each parameter is drawn from a normal distribution with mean zero and variance chosen so that the linear maps have expected norm independent of the hyperparameters.