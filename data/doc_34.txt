For example, we might stipulate that a VALUE has a 75 chance 42One can see examples for English sentences in the Wikipedia article Parse tree. 32 to be a number and a 25 chance to be a variable. With this information, a PCFG defines a probability distribution on strings, which gives zero probability to nongrammatical strings. A symbolic approach to parsing would propose two primary algorithms. One is a parser, which given a grammar and an input produces the parse tree. An other would be an algorithm for learning a grammar from a corpus. Since any finite corpus can be described by many grammars, PCFGs are better suited than CFGs to this problem. In any case the learning and parsing algorithms are not necessarily related. In the connectionist approach followed by LLMs, these two algorithms are subsumed into the definition of a model which can parse any PCFG whose rules are encoded in its weights. By training this on a corpus, the model learns a particular PCFG which generates the corpus. Interpretability as discussed in 7 then means reversing this relation, by extracting a parser and PCFG from the trained model. References 1 Reproduced under the cc by 4.0 license. httpscreativecommons.org licensesby4.0. 2 Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is incontext learning? Investigations with linear models, November 2022. arXiv2211.15661 cs. URL http arxiv.orgabs2211.15661, doi10.48550arXiv.2211.15661. 3 Marie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 1891931, April 2019. URL httpswww.sciencedirect.comsciencearticle piiS1053811919300011, doi10.1016j.neuroimage.2019.01.001. 4 Sanjeev Arora and Boaz Barak. Computational complexity a modern approach. Cambridge University Press, 2009. 5 Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Ris teski. A Latent Variable Model Approach to PMIbased Word Embed dings. arXiv1502.03520 cs, stat, June 2019. arXiv 1502.03520. URL httparxiv.orgabs1502.03520. 6 Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet Autoformalizing and Formally Proving UndergraduateLevel Mathematics, February 2023. arXiv2302.12433 cs. URL httparxiv.orgabs2302.12433, doi 10.48550arXiv.2302.12433. 7 Sebastian Bader and Pascal Hitzler. Dimensions of Neuralsymbolic In arXivcs0511042. tegration A Structured Survey, November 2005. 33 URL httparxiv.orgabscs0511042, doi10.48550arXiv.cs 0511042. 8 Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining Neural Scaling Laws. February 2021. URL https arxiv.orgabs2102.06701v1. 9 Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden Progress in Deep Learning SGD Learns Parities Near the Computational Limit, July 2022. arXiv2207.08799 cs, math, stat. URL httparxiv.orgabs2207.08799, doi10.48550 arXiv.2207.08799. 10 Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117483006330070, 2020. Publisher National Acad Sciences. 11 Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learn ing a statistical viewpoint. arXiv2103.09177 cs, math, stat, March 2021. arXiv 2103.09177. URL httparxiv.orgabs2103.09177. 12 Yonatan Belinkov. Probing classifiers Promises, shortcomings, and advances. Computational Linguistics, 48207219, 2021. URL http arxiv.orgabs2102.12452. 13 Mikhail Belkin.