Explicitly, we postulate an inverse temperature 1T and take28v P w evww evwcid808Here is an observation 99 which supports the idea that word embeddingscontain information about meaning. Since the embeddings are vectors, they canbe added. Consider the following equationking man woman ?928T is the temperature parameter which can be set in say the GPT user interface. Also,this ratio of exponentials is usually called softmax in machine learning as its limitis the argmax function producing a vector whose nonzero components have the same indexvalues as the largest of the inputs.16One might hope that the word which maximizes this inner product is queen,and indeed it is so. There are many more such examples empirically one needsthe dimension p 100 for this to work. One can argue 108, 5 that it followsfrom relations between cooccurence statistics29w,MN w, kingkingMN w, queenqueenMN w, manmanMN w, womanwoman10Given these ideas and a map F from a list of vectors to a vector, we cannow propose a very general class of Lgram autoregressive language models asthe combination of the following steps1. Map the L input words wi to L vectors wi.2. Apply F to the list of these vectors to get a prediction vector v.3. Use the inverse map Eq. 8 to get a probability distribution over words.Furthermore, if the map F has parameters, given a corpus we can determinethem by optimizing the function Eq. 3 with respect to the parameters. And oncewe bring in optimization, we can also optimize with respect to the coefficients ofthe embedding map Eq. 7, so that we can dispense with cooccurence statistics.This is the general prescription followed by the LLMs, and to complete it wejust need to specify a family of maps F . One possibility is to use a general fullyconnected feed forward neural network FFN, also called MLP for multilayerperceptron. We recall that an FFN is a composition of two general types offunctions, linear maps Wi and nonlinear maps , so thatF v Wd Wd1 . . . W1 W0.11In more concrete terms, the maps Wi are multiplication by rectangular matricesof parameters usually called weights in this context, while the maps actindependently on each component of their input vector by a fixed nonlinearfunction such as tanh or more typically ReLU identity for x 0 and zerofor x 0. The main fact we recall about FFNs is that, in the limit that thenumber of parameters becomes large, they can approximate any given functionarbitrarily well 38. We refer the reader interested in learning more to 40, 116.We can get a very natural deep learning version of the Lgram models byusing an FFN for the map F in the prescription above 18. Since this asked fora map from a list of vectors to a vector, we need to convert the input list into asingle vector. This is easy we can take the direct sum of the input vectors, i.e.the dimension L p vector whose components are the concatenated lists of theircomponents.