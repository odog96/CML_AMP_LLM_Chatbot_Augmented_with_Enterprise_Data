There is a huge body of work on this question, and any attempt to review it would rapidly go out of date, but let us review the primary method for studying it. This is benchmarking, the development of standardized sets of test items for which model accuracy can be evaluated in a reproducible way. This is in principle straightforward if the input corresponds to a single correct output, as in multiple choice question answering.19 If the answer is free form text, one can use text comparison metrics such as the ROUGE score. One current standard for evaluating LLMs, BIGbench 127, combines 204 language tasks at first publication they accept new tasks including translation, QA, puzzle solving, text classification and summarization, and tests of common sense reasoning. A leaderboard listing the current best LLMs is at 20. Another is the EleutherAI Language Model Evaluation Harness21 and leaderboard.22 The benchmark suite HELM 80 measures additional metrics such as tendency to repeat copyrighted material, bias, toxicity and the like. 18We are calling this phenomenology following the physics use of the term, not its use in psychology and philosophy to describe the study of subjective experience. 19A potential pitfall is that after a benchmark is published, the test items can find their way into future training data and then be solved by memorization. Methods to detect and prevent this are discussed in the references. 20httpspaperswithcode.comdatasetbigbench 21httpsgithub.comEleutherAIlmevaluationharness 22httpshuggingface.cospacesHuggingFaceH4open llm leaderboard 12 Reasoning ability is of particular interest for mathematical and scientific ap plications of course we all look forward to the day when computers will help us grade assignments, referee papers and do our research. There are many bench marks for solving logical problems expressed in natural language. Benchmarks for mathematical theorem proving include NaturalProofs 135, MiniF2F 144 and ProofNet 6 as of mid2023 LLMs and the best other systems can find many proofs 2080 but still fail on some seemingly easy cases. Simpler as pects of reasoning which have benchmarks are the ability to deal with negation 142, consistency between different phrasings of the same question 61, and compositionality the ability to analyze statements and problems into simpler parts, solve these and combine the results 111. Natural language tasks are very complex, and benchmarks constructed from real world data cannot be used directly in theoretical considerations. For this purpose one generally defines toy worlds and generates synthetic data. The possibilities are endless, but some which have been used are arithmetic prob lems decimal arithmetic modular arithmetic, game play, solving systems of equations, and parsing formal languages. A particularly interesting task is lin ear regression 48 since this is the prototypical case of statistical inference, a system which learns to do it can be said to be learning how to learn. Coming to scaling laws, denote the model size number of parameters as P and the dataset size number of tokens in the corpus as D, then there are two general regimes.