This is a well understood concept for metric spaces 92, which was implicit in the discussion of word embeddings in 5. There the simplest construction the cooccurence matrix produced vectors with one component for each word, but by projecting on a subspace one could greatly reduce this dimension with little loss in accuracy. The generalization of these ideas to neural networks seems important. Once one believes an LLM is carrying out a task using a particular circuit or CM, one can go on to ask how it learned this implementation from the data. One can get theoretical results in the limit of infinite training data andor for simple tasks in which the dataset is constructed by a random process. Learning 39One can rewrite any grammar to have this property Chomsky normal form by introduc ing more nonterminals. 24 in transformer models trained on realistic amounts of data is mostly studied empirically and using synthetic data. A few recent interesting works are 2, 51, 103. Intuitively one expects that simpler instances of a task are learned first, allowing the model to learn features which are needed to analyze more complex instances, and there is a lot of evidence for this. The idea that many submodels can be learned simultaneously, including straight memorization and submodels which rely on structure, also seems important. Ultimately learnability is crucial but we should keep in mind that in analogous questions in physics, evolution, and so on, it is much easier to understand optimal and critical points in the landscape than to understand dynamics. This brings us to incontext learning, the ability of an LLM to perform diverse tasks given only a few examples of inputoutput pairs. The simplest hypothesis is that the model has learned the individual tasks, and the examples are selecting a particular task from this repertoire. It has been argued that this is guaranteed to happen in the infinite data limit for a model trained on a mixture of tasks 139, 136. If the many tasks have common aspects for example parsing might be used in any linguistic task, one can ask how the model takes advantage of this, a question discussed in 54. Understanding LLMs is a very active research area and there is much more we could say, but let us finish by summarizing the two main approaches we described. One can postulate a representation and a computation designed to perform a task, and look for evidence that the LLM actually uses the postu lated structure. Alternatively, one can look for a function in some simpler class such as digital circuits which well approximates the function computed by the transformer model, and then reverse engineer the simpler function to find out what it is doing. Either or both of these procedures could lead to inter pretable systems and if so, are answers to the question what has the LLM learned.