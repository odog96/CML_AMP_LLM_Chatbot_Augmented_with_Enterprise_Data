If distance from the observer is explicitly representedand used in reasoning, then it becomes hard to get this type of question wrong.Conversely, to the extent that LLMs do get it wrong, this might be evidencethat they lack this type of world model or cannot effectively use it.There are many papers exhibiting LLM errors and suggesting such interpretations, but often one finds that next years model does not make the sameerrors. At the present rate of progress it seems premature to draw any strongconclusions. My own opinion is that there is no barrier in principle to LLMsconstructing internal nonlinguistic models of the world, and the work 78 onOthelloGPT discussed in 7 is a nice demonstration of what is possible. Thisis not to say that any and all models can be learned, but rather that it mightbe better for now to focus on other significant differences between LLM andhuman reasoning, of which there are many. I will come back to this below.If LLMs and other connectionist systems do not work in the same way asbrains, what other guidance do we have? In 7 we discussed one answer, thehypothesis that they work much like the algorithms and circuits studied in41Chomsky rejects this idea, saying that The childs operating system is completely different from that of a machine learning program. New York Times, March 8, 2023.27computer science. Perhaps trained LLMs implement algorithms like those designed by computational linguists, or perhaps new algorithms which were notpreviously thought up but which can be understood in similar terms. In eitherversion this is still a hypothesis, but if we grant it we can draw on insights fromtheoretical computer science which apply to all such algorithms.Computational complexity theory 4, 137 makes many statements and conjectures about how the time and space required by a particular computationdepends on the size of the problem usually meaning the length of the input.The most famous of these, the P NP conjecture, states very loosely that forproblems which involve satisfying general logical statements, finding a solutioncan be much harder than checking that the solution is correct.From this point of view, a central question is the complexity class of circuitswhich can be realized by constant depth transformers, meaning that the numberof layers does not grow with the window size. Roughly, this is the complexityclass TC0 of constant depth circuits with threshold gates 28, 41, 95, 96. Ofcourse in an autoregressive LLM one can repeat this operation to compute asequence of words thus the circuit defines the transition function of a finitestate machine FSM where the state is the window, and the LLM has learnedto simulate this FSM. If a natural algorithm to perform a task is in a moredifficult complexity class than the FSM can handle, this is a reason to think thetask cannot be learned by this type of LLM.