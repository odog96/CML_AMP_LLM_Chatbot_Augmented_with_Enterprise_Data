wL Lcid89i1P wi P w number of occurrences of w in the corpustotal number of words in the corpus.1Of course, this model captures very little of the structure of language, whichinvolves dependencies between the word choices.LLMs are generative models,11 by which we will mean that there is a practical method for sampling from the distribution. To explain this, consider a wordprediction task in which some words in a string are given the input andothers left blank the output. Given a probability distribution P w1 . . . wL,there is a corresponding conditional probability distribution for the output giventhe input. As an example, suppose we are given the string The cat BLANKoutside, where BLANK is a token which marks the position of the missingword. The relevant conditional probabilities might beP the cat went outside the cat BLANK outside 0.5P the cat sat outside the cat BLANK outside 0.2and so on, summing to total probability 1. In the masked word prediction task,the model must determine or sample from this distribution.A particularly convenient case is to give the conditional probability of theword which follows a given string, which we denote asP wn1 w1 w2 . . . wn1 wn.2By sampling this distribution to get a new word wn1 and appending it to theend, the string can be extended one word at a time. Repeating this processgives an arbitrarily long string, which by the laws of probability is a samplefrom the original probability distribution P w1 . . . wL, for exampleP the cat went outside P theP cat theP went the catP outside the cat went.This factorization of the probability into successive conditional probabilitiesdefines the class of autoregressive models. One could furthermore require that11Many different definitions of this term can be found in the literature.8the conditional probability Eq. 2 depends only on the k most recent words, inwhich case one would have a Markov model whose state is a string of k words.To evaluate how good a language model is, we want to quantify how well itsprobability distribution approximates that of the corpus the empirical distribution. The standard measure of this is the cross entropy. For an autoregressivemodel this is a sum of terms, one for each word in the corpus,12L 1NN ncid88i1log P win wi wi1 . . . win13One also refers to exp L as perplexity. In a machine learning approach, wecan use Eq. 3 as an objective function and minimize it as a function of thenetwork parameters to train the network. We can then apply the many tools ofML backpropagation, splitting the sum into batches, varying the learning rateand so on, to get an efficient and effective model. While the details are an artwhich depends on the particular domain and model architecture,13 conceptuallythese are much the same for LLMs as for other machine learning models.This statistical approach to modeling language has been pursued since thelate 80s 21, 64, 87 and many models were developed, such as the recurrent neural network RNN which we will describe in 5.