Code is a good source of rules because almost all of it has been debugged, leading to rules which are correct in their original context of course they might not be correctly applied. It is a longstanding question whether internal representations both in AI and in humans are shared between different natural languages it would be truly fascinating to know how much they are also shared with code. If this intuition is right, then LLMs reasoning capability might be improved by training on far more code and other content which is guaranteed to be correct. Such content could be generated synthetically as tautologies, or even better as formal verified mathematics as proposed in 129. Here is a different point of view the problem is not that the systems make things up, after all creativity has value. Rather, it is that they do not provide much indication about the confidence to place in a particular output, and do not have ways to adapt their reasoning to statements known at different levels of confidence. Much of our reasoning involves uncertain claims and claims which turn out to be false, the point is to distinguish these from justified claims and keep track of our confidence in each belief. While it is possible to extract confidence scores from LLMs 65, there is also a philosophical point to make here not all facts have the same epistemological status. Some facts are grounded in evidence others are true by definition. LLMs are of course statistical models. Even for a completely deterministic task, say doing arithmetic, a statistical approach to learning is very powerful. This is because learning based on inputs which consist of finitely many training examples, given in a random order, is naturally formulated in statistical terms. But without making additional nonstatistical assumptions, one can never go from almost 100 confidence to 100 confidence. This difference is crucial in many aspects of human thought. Of course, logical reasoning and mathematics stand out as prime examples. Long chains of reasoning are only possible if the individual links are reliable. But it is also crucial in social reasoning. There is an essential difference between statistical and evidencebased statements, say Michael is a popular name, and tauto logical, definitional and descriptive statements such as My name is Michael. While the first statement might be a subject of discussion, a model which can get confused about the second statement is clearly missing a defining aspect of human thought, and will lose the confidence of its interlocutor. Perhaps episte mological status and tautological correctness need to be somehow represented in the model. It need not be designed in, but the model needs to be given additional signals beyond next word prediction to learn it. The third point on my list, reflection, does not seem to be much discussed, but to me seems just as important. In computer science, reflection is the ca pability of a system to work with its programs as a form of data 124, 138.