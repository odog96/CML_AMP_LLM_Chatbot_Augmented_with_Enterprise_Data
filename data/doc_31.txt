Thecontextfree grammars are the right degree of complexity for many purposes. Inparticular, programming languages and the formal languages of mathematicallogic can be described using CFGs and thus the algorithms for working withthem and associated theory are well developed.Besides recognizing and parsing languages, one can describe other linguistictasks in similar terms. A trivial example would be word replacement, withrules such as OLDi NEWi. Realistic tasks benefit from frameworks withmore structure. For example, to use the grammar in Eq. 17 to do arithmetic,we would be much better off with a framework in which the token VALUEcarries an associated numerical or symbolic value. This can be done with theframework of attribute grammars. When we suggest in 8 that LLMs performnatural language tasks using systems of large numbers of rules, we have thissort of extended grammatical framework in mind.CFGs are not really adequate for natural languages, with their inherentambiguity and their many special cases and exceptions. A more general formalism is the probabilistic CFG. This is obtained by associating a probabilitydistribution to each symbol which appears on the left hand side of a rule thenonterminals. For example, we might stipulate that a VALUE has a 75 chance42One can see examples for English sentences in the Wikipedia article Parse tree.32to be a number and a 25 chance to be a variable. With this information, aPCFG defines a probability distribution on strings, which gives zero probabilityto nongrammatical strings.A symbolic approach to parsing would propose two primary algorithms. Oneis a parser, which given a grammar and an input produces the parse tree. Another would be an algorithm for learning a grammar from a corpus. Since anyfinite corpus can be described by many grammars, PCFGs are better suitedthan CFGs to this problem. In any case the learning and parsing algorithmsare not necessarily related.In the connectionist approach followed by LLMs, these two algorithms aresubsumed into the definition of a model which can parse any PCFG whose rulesare encoded in its weights. By training this on a corpus, the model learns aparticular PCFG which generates the corpus. Interpretability as discussed in7 then means reversing this relation, by extracting a parser and PCFG fromthe trained model.References1 Reproduced under the cc by 4.0 license. httpscreativecommons.orglicensesby4.0.2 Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and DennyZhou. What learning algorithm is incontext learning? Investigationswith linear models, November 2022. arXiv2211.15661 cs. URL httparxiv.orgabs2211.15661, doi10.48550arXiv.2211.15661.3 Marie Amalric and Stanislas Dehaene. A distinct cortical network formathematical knowledge in the human brain. NeuroImage, 1891931,April 2019. URL httpswww.sciencedirect.comsciencearticlepiiS1053811919300011, doi10.1016j.neuroimage.2019.01.001.4 Sanjeev Arora and Boaz Barak. Computational complexity a modernapproach. Cambridge University Press, 2009.5 Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A Latent Variable Model Approach to PMIbased Word Embeddings. arXiv1502.03520 cs, stat, June 2019. arXiv 1502.03520. URLhttparxiv.orgabs1502.03520.6 Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W.Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet Autoformalizingand Formally Proving UndergraduateLevel Mathematics, February 2023.arXiv2302.12433 cs. URL httparxiv.orgabs2302.12433, doi10.48550arXiv.2302.12433.7 Sebastian Bader and Pascal Hitzler.