A canonical example is image recognition say, given an array of pixels light intensity values, estimate the probability that the image depicts a cat. Rather than design a system to do this, one starts with a large dataset of images with labels cat and noncat. One then designs a very general statistical model and trains it on this dataset to predict the label given the image. This is supervised learning, one can also do selfsupervised learning in which the system predicts some part of the data given other parts say, filling in part of an image. A third standard ML paradigm is reinforcement learning, which applies to tasks which involve choosing actions to fulfill a longer term goal. The classic example is game playing, as in AlphaGo and AlphaZero. In any case, since the problem is formulated statistically, it is possible to 7httpswww.sigsam.org 8httpscyc.com 9I first learned about this from 83, 101. 5 consider the training dataset one item at a time, and use it to incrementally improve the model. This is almost always done by formulating the task in terms of an objective function which measures the quality with which it is performed, for example the accuracy with which correct labels are assigned to images. One then takes a parameterized model and trains it by optimizing this function, evaluated on the training dataset, with respect to the model parameters. For the classic models of statistics this can be done analytically, as in a least squares fit. For more general models one uses numerical methods, such as gradient descent. Either way, a central question of statistics and machine learning is generalization, meaning the extent to which the model well describes data not in the training set but sampled from the same probability distribution. A well known principle which speaks to this question is Occams razor, that simpler models will generalize better. This is often simplified to the rule that a model should have the minimal number of parameters needed to fit the dataset. Not all machine learning systems are deep learning 76 or connection ist 118. These terms generally refer to the use of neural networks with large numbers of parameters which provide effective universal function approxima tors. While the idea is very old 117, before 2012 it was widely believed to be impractical. One argument for this was the dull side of Occams razor mod els with so many parameters were destined to overfit and would not generalize. Evidently this is not the case, leading to concepts such as benign overfitting. 10, 15 Another argument was that the objective functions for these models are highly nonconvex and optimization would get stuck at poor quality local minima. This can be a problem, but turns out to be solvable for reasons that are partially understood 31, 49.