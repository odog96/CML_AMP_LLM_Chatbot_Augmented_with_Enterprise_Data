To motivate and efficiently carry out such costly experiments, one needs some ability to predict in advance how changes in model and dataset size will affect the training methods for example the optimal choice of learning rate and performance. An important advance in this direction was the observation of power law scaling in language model performance 67. Figure 2 plots the test loss16 against the logarithms of the sizes and compute resources used, and these straight lines correspond to a power law relation between size and perplexity. This scaling holds over many decades in model size and, while the exponents 0.076 to 0.095 are rather small, this is a strong argument that larger models will have better performance. These ideas were also used to determine optimal model dataset size tradeoff 58 and the scaling of hyperparameters 140. These results were a significant input into the decision to do this very expensive research. Year Model Number of Parameters Dataset size tokens 2018 110M GPT 2018 BERT 340M 1.5B 2019 GPT2 175B 2020 GPT3 2022 PaLM 540B 2023 GPT4 1.4T ? 1B 3B 10B 500B 780B ? Table 1 Large Language Models MBT millionbilliontrillion. In many cases several model sizes were considered we quote the largest. 14State of the art, in other words an improvement over all previously evaluated models. 15httpscommoncrawl.org 16This is Eq. 3 minus log perplexity evaluated on texts which were removed or held out of the training set, to get a measure of generalization ability. 10 Figure 2 Language modeling performance as a function of model size, dataset size, and amount of compute used for training. From Kaplan et al, Scaling Laws for Neural Language Models, 2020 67. Now it should be realized that, while the measure being improved here is fairly objective, still there was no strong reason to think that improving it would lead to models with qualitatively new emergent capabilities. But it appears that this is what happened GPT3 and its finetuned cousins such as Codex were able to do tasks, such as write computer code from a natural language description, for which smaller models were almost worthless.17 We will discuss more of this progress shortly, and speculate a bit in the conclusions. One of the most interesting LLM phenomena is incontext learning, first discussed in the original GPT3 paper 22. This refers to the ability of an LLM to carry out tasks different from its original objective without modify ing its parameters, indeed without any need for additional training on the new task fine tuning. Rather, after being given as input text a few examples of inputoutput pairs, the LLM can be given another input and will generate a suitable output. Say the new task is question answering, then after a few questionanswer examples the LLM will answer the next question it is given.