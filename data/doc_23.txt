The second is compression, thatembeddings and circuits which are simple and interpretable can be mapped intomore randomlooking lower dimensional forms. This is a well understoodconcept for metric spaces 92, which was implicit in the discussion of wordembeddings in 5. There the simplest construction the cooccurence matrixproduced vectors with one component for each word, but by projecting on asubspace one could greatly reduce this dimension with little loss in accuracy.The generalization of these ideas to neural networks seems important.Once one believes an LLM is carrying out a task using a particular circuitor CM, one can go on to ask how it learned this implementation from the data.One can get theoretical results in the limit of infinite training data andor forsimple tasks in which the dataset is constructed by a random process. Learning39One can rewrite any grammar to have this property Chomsky normal form by introducing more nonterminals.24in transformer models trained on realistic amounts of data is mostly studiedempirically and using synthetic data. A few recent interesting works are 2, 51,103. Intuitively one expects that simpler instances of a task are learned first,allowing the model to learn features which are needed to analyze more complexinstances, and there is a lot of evidence for this. The idea that many submodelscan be learned simultaneously, including straight memorization and submodelswhich rely on structure, also seems important. Ultimately learnability is crucialbut we should keep in mind that in analogous questions in physics, evolution,and so on, it is much easier to understand optimal and critical points in thelandscape than to understand dynamics.This brings us to incontext learning, the ability of an LLM to performdiverse tasks given only a few examples of inputoutput pairs. The simplesthypothesis is that the model has learned the individual tasks, and the examplesare selecting a particular task from this repertoire.It has been argued thatthis is guaranteed to happen in the infinite data limit for a model trainedon a mixture of tasks 139, 136. If the many tasks have common aspects forexample parsing might be used in any linguistic task, one can ask how themodel takes advantage of this, a question discussed in 54.Understanding LLMs is a very active research area and there is much morewe could say, but let us finish by summarizing the two main approaches wedescribed. One can postulate a representation and a computation designed toperform a task, and look for evidence that the LLM actually uses the postulated structure. Alternatively, one can look for a function in some simpler classsuch as digital circuits which well approximates the function computed by thetransformer model, and then reverse engineer the simpler function to findout what it is doing. Either or both of these procedures could lead to interpretable systems and if so, are answers to the question what has the LLMlearned. There is no guarantee that they will work and it might turn out thatone cannot understand LLMs without new ideas, but they deserve to be tried.8 Questions and discussionLarge language models have revolutionized computational linguistics and openedup many new applications of AI.