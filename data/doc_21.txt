A pretty example ofprobing for a world model is the recent work of Li et al 78 see also 130 onrepresentations in a transformer model trained to play the board game Othello.3737For readers not familiar with this game, two players alternate in placing black and white22They train a model OthelloGPT 38 to take as input a sequence of 60 legalmoves, for example E3 D3 ... in the standard algebraic notation, and at eachstep to predict the next move. The trained model outputs only legal moveswith very high accuracy, and the question is whether this is done using internalrepresentations which reflect the state of the game board, say the presenceof a given color tile in a given position. Following the probe paradigm, theyobtain FFNs which, given intermediate activations, can predict whether a boardposition is occupied and by which color tile. Furthermore, after modifyingthe activations so that the FFNs output has flipped a tile color, the modelpredicts legal moves for the modified board state, confirming the identification.Neuroscientists can only dream of doing such targeted experiments.Numerous probe studies have been done on LLMs. One very basic question is how they understand grammatical roles and relations such as subject,object and the like. This question can be sharpened to probing their internalrepresentations for parse trees, a concept we review in the appendix. To getthe targets for the probe, one can use a large dataset of sentences labeled withparse trees, the Penn Treebank 90. This was done for BERT in 27, 56, 88 bythe following procedure denote the embedding in a fixed layer of word i asui, then the model learns a projection P on this space, such that the distancesdi, j P ui uj in this inner product well approximate the distance between words i and j defined as the length of the shortest path connecting themin the parse tree. For BERT with d 1000 this worked well with a projectionP of rank 50.Once one knows something about how information is represented by themodels, one can go on to try to understand how the computations are done. Oneapproach, also analogous to neuroscience, is to look for specific circuits whichperform specific computations. An example of a circuit which appears in trainedtransformer models is the induction head 42, 107. This performs the followingtask given a sequence such as A B . . . A it predicts a repetition, in thisexample B. The matching between the tokens the two As in the example isdone by attention. A number of works have proposed and studied such circuits,with various motivations and using various theoretical lensesinterpretabilityand LLMs 106, incontext learning 107, 2, formal language theory 94, 28,computational complexity theory 41, 82, etc..Reverse engineering a large network ab initio, i.e. with minimal assumptionsabout what it is doing, seems challenging, but maybe automated methods will bedeveloped 33, 46.