In the analogy, perplexity is playing a similar role as anobjective measure of language model performance defined independently of themore interesting qualitative behaviors which reflect intelligence.How far can one push this analogy? Could perplexity be as central to language as energy is to physics? Eq. 3 has a fairly objective definition, so theidea is not completely crazy. But, not only was its relation to performance onactual tasks not predictable in advance, even after the fact clear thresholds orother signals for emergence of tasks have not yet been identified 133. Perhapsif there are universal thresholds, evidence for them could be seen in humans.25More likely, additional variables the quality and nature of the training corpus,24In 5 we explain how text can be thought of embedded in a high dimensional space.25Thanks to Misha Tsodyks for this suggestion.14details of the tasks, etc. would need to be controlled to see them. This isanother question probably better studied in simpler tasks using synthetic data.The final topic we discuss is the behavior of the objective function Eq. 3as a function of training time.26 In almost all ML runs, such a plot shows longplateaus interspersed with steep drops. This has been interpreted in many ways,ranging from evidence about the nature of learning, to a simple consequence ofrandomness of eigenvalues of the Hessian of the loss function. A more recentobservation is to compare training and testing accuracy on the same plot. In9, 110 it was argued that these two metrics improve at two distinct stages oftraining. First, the model memorizes training examples. Later, it generalizesto the testing examples. This grokking phenomenon has been suggested asevidence for learning of circuits 103, an idea we discuss in 7.5Simpler language modelsHere we describe a few generative language models in detail to fix the concepts.As points of notation, let W be the set of words or, if the reader prefers,numbers which index a position in a list of words. We denote the cardinality ofa set S as S, so W is the number of distinct words. The space of N componentreal vectors is denoted RN .The simplest model is the Ngram model defined in terms of the conditionalprobabilitiesP wN w1 w2 . . . wN 1,5which are all taken to be independent. Given this minimalist assumption, aplausible way to estimate them from the corpus isP wN w1 w2 . . . wN 1 Number of occurrences of w1 w2 . . . wN 1 wNw Number of occurrences of w1 w2 . . . wN 1 wcid80.6This simple model with N 3 or 4 works better than one might think see examples in 64 and can be improved a bit by simple statistical tricks smoothing.But the exponential growth of the number of strings in N means that there isno hope of taking N large enough to model even a single paragraph.