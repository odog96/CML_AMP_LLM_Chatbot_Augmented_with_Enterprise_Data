Perhaps related to this,brains are not feedforward networks but have many bidirectional connections.Whatever brains are doing, it works very well LLMs like other current deeplearning systems need far more training data than humans. Furthermore, theLLMs we discussed do not interact with the world. Some argue that on philosophical grounds, a model trained only on language prediction can never learnmeaning 16. While I do not find this particular claim convincing, I agree thatwe should not assume that LLMs perform tasks the same way humans do. Stillboth similarities and differences are interesting can we make the analogies withcognitive psychology more precise?One analogy 17, 50, is with the well known concept of fast and slow thinking in behavioral psychology 66. To summarize, humans are postulated tohave two modes of thought, system 1 which makes fast, intuitive judgments,40What magical trick makes us intelligent? The trick is that there is no trick. The powerof intelligence stems from our vast diversity, not from any single, perfect principle. 10026and system 2 which can focus attention and do calculations, logic, and planning. While system 2 is more general and less errorprone, using it requiresconscious attention and effort. According to the analogy, LLMs implement system 1 thinking, and are weak at system 2 thinking.In 84 it is argued that LLMs have formal linguistic competence but notfunctional competence. In plainer terms, they are solving problems by manipulating language using rules, but they lack other mechanisms of human thought.While it may be surprising that a purely rulebased system could do all thatLLMs can do, we do not have a good intuition about what rulebased systemswith billions of rules can do.What are the other mechanisms? There is a longstanding hypothesis in cognitive science, modularity of mind 45, according to which the human brain hasmany mental modules with different capabilities. These include a languagemodule of the sort that Chomsky famously advocated and many others, including one for geometric and physical reasoning, another for social reasoning andtheory of mind, and perhaps others. Notably, formal logic and mathematicalreasoning seem to call upon different brain regions from those which specializein language 3, suggesting that these functions are performed by different mental modules. One can thus hypothesize that LLMs have commonalities with thehuman language module and might be useful scientific models for it,41 but thatprogress towards human level capability will eventually stall without analogs ofthe other modules. 73A related claim is that current LLMs, even when they perform well on benchmarks, do not construct models of the world. Consider reasoning about spatialrelations for example if A is in front of B is in front of C, then A is in front ofC. Such reasoning is greatly facilitated by representing the locations of objectsin space, perhaps in terms of coordinates, perhaps using place cells or in someother nonlinguistic way.