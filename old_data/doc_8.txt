As an example, suppose we are given the string The cat BLANK outside, where BLANK is a token which marks the position of the missing word. The relevant conditional probabilities might be P the cat went outside the cat BLANK outside 0.5 P the cat sat outside the cat BLANK outside 0.2 and so on, summing to total probability 1. In the masked word prediction task, the model must determine or sample from this distribution. A particularly convenient case is to give the conditional probability of the word which follows a given string, which we denote as P wn1 w1 w2 . . . wn1 wn. 2 By sampling this distribution to get a new word wn1 and appending it to the end, the string can be extended one word at a time. Repeating this process gives an arbitrarily long string, which by the laws of probability is a sample from the original probability distribution P w1 . . . wL, for example P the cat went outside P theP cat theP went the catP outside the cat went. This factorization of the probability into successive conditional probabilities defines the class of autoregressive models. One could furthermore require that 11Many different definitions of this term can be found in the literature. 8 the conditional probability Eq. 2 depends only on the k most recent words, in which case one would have a Markov model whose state is a string of k words. To evaluate how good a language model is, we want to quantify how well its probability distribution approximates that of the corpus the empirical distribu tion. The standard measure of this is the cross entropy. For an autoregressive model this is a sum of terms, one for each word in the corpus,12 L 1 N N n cid88 i1 log P win wi wi1 . . . win1 3 One also refers to exp L as perplexity. In a machine learning approach, we can use Eq. 3 as an objective function and minimize it as a function of the network parameters to train the network. We can then apply the many tools of ML backpropagation, splitting the sum into batches, varying the learning rate and so on, to get an efficient and effective model. While the details are an art which depends on the particular domain and model architecture,13 conceptually these are much the same for LLMs as for other machine learning models. This statistical approach to modeling language has been pursued since the late 80s 21, 64, 87 and many models were developed, such as the recurrent neu ral network RNN which we will describe in 5. Following the general machine learning experience that supervised tasks learning from inputoutput pairs are easier than unsupervised tasks, many of these works addressed machine translation and parsing, for which there are good labeled datasets documents with their translations sentences with their grammatical structure.