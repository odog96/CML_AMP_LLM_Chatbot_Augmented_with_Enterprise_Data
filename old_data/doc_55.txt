One of these is an FFN as previously discussed, but now appliedto each embedding vector independently, so vi FF F N ui.30Other reviews explaining these definitions include 109, 131.18The other layer type is called attention, and it is defined as followsui vi Wicid88ci,jujci,j j1exp ui B ujj1 exp ui B ujcid80i1314where B is a learnable matrix whose elements are model parameters equivalently, u B v is a bilinear form and W is a linear map also learnable.In words, an item vi in the output vector is a linear transformation of aweighted sum of the inputs uj with i j and can depend on any of them.31The weights ci,j are given by a softmax or Boltzmann weight just as in Eq.8. Thus there is a very general learnable way for each output to choose whichof the input vectors are most useful as inputs. Suppose the product u B v isthe dot product, then attention selects the input components uj most similarto the current units input, uj ui in the notation of 5. The matrix B allowsfor comparing different parts of the embeddings and ignoring other parts, in away determined by optimizing the objective function Eq. 3.Composing these two types of functions or layers produces a map fromRpL to RpL. Often one takes, instead of the pure FFN and attention functions, sums of these with the identity function residual connections. The FFNsgenerally have a single hidden layer which can be of a different dimension, callthis ph.32 Finally, while the language model prescription asked for a map to Rp,this is easily obtained by just taking the last vector in the final output list.There are two more essential details to cover and many minor details wewill skip. The first is the concept of attention head. The definition Eq. 13allowed for a general linear transformation W whose range is the output vector.We are free to choose its dimension, call it q, and typically one takes this toIn return one can use manybe much less than the embedding dimension p.copies of Eqs. 13,14 in parallel with different choices for B and W , to producemany outputs. One can then concatenate these outputs to get a final output ofdimension p. These copies are called attention heads and we will denote theirnumber by H, so p Hq.The second essential detail is that, so far, there is nothing in the definitionthat keeps track of the order of the list of input vectors the output of Eq. 13will be invariant under a general permutation of the input vectors. While thisis an elegant property, it is not what we want for processing language, for whichthe order of the words matters. The cure for this is very simple one takes asinputs not the word embeddings Eq. 7, but the direct sum concatenation ofthese with positional embedding vectors, i.e. vectors which encode the positionindex of the word in the string.