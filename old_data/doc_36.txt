04511. 28 David Chiang, Peter Cholak, and Anand Pillay. Tighter Bounds on the Expressivity of Transformer Encoders, May 2023. arXiv2301.10743 cs. URL httparxiv.orgabs2301.10743, doi10.48550arXiv. 2301.10743. 29 Ted Chiang. Chatgpt is a blurry jpeg of the web. The New Yorker, February 2023. 30 Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating Long Sequences with Sparse Transformers. April 2019. URL https arxiv.orgabs1904.10509v1. 31 Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The Loss Surfaces of Multilayer Networks, January 2015. arXiv1412.0233 cs. URL httparxiv.orgabs1412.0233, doi10.48550arXiv.1412.0233. 32 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. PaLM Scal ing Language Modeling with Pathways. arXiv2204.02311 cs, April 2022. arXiv 2204.02311. URL httparxiv.orgabs2204.02311. 33 Bilal Chughtai, Lawrence Chan, and Neel Nanda. A Toy Model of Uni versality Reverse Engineering How Networks Learn Group Operations, May 2023. arXiv2302.03025 cs, math. URL httparxiv.orgabs 2302.03025, doi10.48550arXiv.2302.03025. 34 Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical Foundations for a Compositional Distributional Model of Meaning, March 2010. arXiv1003.4394 cs, math. URL httparxiv.orgabs1003. 4394, doi10.48550arXiv.1003.4394. 35 Taco Cohen and Max Welling. Group equivariant convolutional net works. In International conference on machine learning, pages 29902999. PMLR, 2016. arXiv1602.07576. 36 Remi Coulom. Efficient selectivity and backup operators in montecarlo tree search. In International conference on computers and games, pages 7283. Springer, 2006. 36 37 Francis Crick. The recent excitement about neural networks. Nature, 337129132, 1989. 38 George V. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2303314, 1989. 39 Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. BERT Pretraining of Deep Bidirectional Transformers for Language Un derstanding. October 2018. arXiv 1810.04805. URL httpsarxiv. orgabs1810.04805v1. 40 Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural Network Approximation. arXiv2012.14501 cs, math, December 2020. arXiv 2012.14501. URL httparxiv.orgabs2012.14501. 41 Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive Biases and Variable Creation in SelfAttention Mechanisms. arXiv2110.10090 cs, stat, October 2021. arXiv 2110.10090. URL httparxiv.orgabs2110.10090. 42 Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac HatfieldDodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy Models of Superposition, September 2022. arXiv2209.10652 cs. URL http arxiv.orgabs2209.10652, doi10.48550arXiv.2209.10652. 43 Philip Feldman, James R. Foulds, and Shimei Pan. Trapping LLM Hal lucinations Using Tagged Context Prompts, June 2023. arXiv2306.06085 cs. URL httparxiv.orgabs2306.06085, doi10.48550arXiv. 2306.06085. 44 John Rupert Firth. Studies in linguistic analysis. WileyBlackwell, 1957. 45 Jerry A Fodor. The modularity of mind. MIT press, 1983. 46 Dan Friedman, Alexander Wettig, and Danqi Chen. Learning Transformer Programs, June 2023. arXiv2306.01128 cs. URL httparxiv.org abs2306.01128, doi10.48550arXiv.2306.01128. 47 Artur dAvila Garcez and Luis C. Lamb. Neurosymbolic AI The 3rd Wave, December 2020. arXiv2012.05876 cs. URL httparxiv.org abs2012.05876. 48 Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What Can Transformers Learn InContext? A Case Study of Simple Function Classes, January 2023. arXiv2208.01066 cs. URL httparxiv.org abs2208.01066, doi10.48550arXiv.2208.01066. 37 49 Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. 50 Anirudh Goyal and Yoshua Bengio.