There is no guarantee that they will work and it might turn out that one cannot understand LLMs without new ideas, but they deserve to be tried. 8 Questions and discussion Large language models have revolutionized computational linguistics and opened up many new applications of AI. Understanding how they work is both straight forward we explained it in 6 and at the same time an outstanding scientific challenge. This is because the question how do they work has multiple mean ings. On the one hand, LLMs are a relatively simple solution to the task of predicting the likely next word in a text. On the other hand, they also seem to perform many other tasks which require intelligence, such as solving the physics word problem in Figure 1. While we do not have a strong understanding of what a system which can perform these tasks must do, a vast body of work in cognitive science and AI supports ones first naive intuition that such a system must be doing sophisticated analyses of language, must contain models of the real world, and must be able to do fairly general logical reasoning. Before it was demonstrated, the idea that all this could be learned as a byproduct of 25 word prediction would have seemed hopelessly optimistic, had anyone dared to suggest it. Extraordinary claims should be greeted with skepticism. One must guard against the possibility that a successful ML system is actually picking up on superficial aspects or statistical regularities of the inputs, the clever Hans effect. Addressing this is an important function of the benchmark evaluations discussed in 4. Of course as LLMs get good at performing tasks of practical value, the skeptical position becomes hard to maintain. Intelligence and language are incredibly complex and diverse. According to Minsky,40 this diversity is a defining feature of intelligence. The goal of under standing LLMs or any general AI will not be accomplished by understanding all of the content in their training data, the entire internet. Rather, the trick we need to understand is how a single system can learn from this diverse corpus to perform a wide range of tasks. Theories of what is learnable are a central part of computer science 68. Although theoretical understanding has a long way to go to catch up with LLM capabilities, for simpler and better understood tasks much is known. In these notes we mostly looked at this question through the lens of computer science, and took as the gold standard for explaining how an LLM learns and performs a task, a computational model expressed as an algorithm or a circuit together with arguments that the trained LLM realizes this model. This point of view has many more insights to offer, but before we discuss them let us consider some other points of view. In 7 we drew the analogy between detailed study of transformer circuits and neuroscience what others can we consider? Another analogy is with cognitive psychology.