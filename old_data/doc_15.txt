5 Simpler language models Here we describe a few generative language models in detail to fix the concepts. As points of notation, let W be the set of words or, if the reader prefers, numbers which index a position in a list of words. We denote the cardinality of a set S as S, so W is the number of distinct words. The space of N component real vectors is denoted RN . The simplest model is the Ngram model defined in terms of the conditional probabilities P wN w1 w2 . . . wN 1, 5 which are all taken to be independent. Given this minimalist assumption, a plausible way to estimate them from the corpus is P wN w1 w2 . . . wN 1 Number of occurrences of w1 w2 . . . wN 1 wN w Number of occurrences of w1 w2 . . . wN 1 w cid80 . 6 This simple model with N 3 or 4 works better than one might think see exam ples in 64 and can be improved a bit by simple statistical tricks smoothing. But the exponential growth of the number of strings in N means that there is no hope of taking N large enough to model even a single paragraph. The entire internet contains in order of magnitude 1012 words, and such a corpus will contain only a vanishingly small fraction of the likely twenty word strings.27 A more general principle which we can take from the Ngram model is the dis tributional hypothesis, which has been pithily summarized as you shall know a word by the company it keeps. 44 In other words, by proper use of the statistics of neighboring words, one can define quantities which capture prop erties and even the meanings of words. The simplest expression of this idea 26This is roughly the time in which the gradient descent operates, see Eq. 16. In LLMs one often considers each data item only once in a training run, so it is related to but different from dataset size. 27Statistical estimates of perplexity are in the 100s, and the best current LLMs have per plexity 20. 15 is the cooccurrence matrix. Before explaining this, let us mention a detail of practical systems, which in place of words use tokens, meaningful components of words. A physics illustration is the word supersymmetrization. Even for a nonphysicist reader encountering it for the first time, this word naturally breaks up into super, symmetry and ization, pieces which appear in many words and which are called tokens. And not only does this decomposition apply to many words, it helps to understand their meaning. This process of replacing single words by strings of tokens tokenization is a first step in LLM pro cessing, and henceforth when we say word we will mean word or token in this sense.