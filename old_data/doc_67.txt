While it is possible to extractconfidence scores from LLMs 65, there is also a philosophical point to makehere not all facts have the same epistemological status. Some facts are groundedin evidence others are true by definition.LLMs are of course statistical models. Even for a completely deterministictask, say doing arithmetic, a statistical approach to learning is very powerful.This is because learning based on inputs which consist of finitely many trainingexamples, given in a random order, is naturally formulated in statistical terms.But without making additional nonstatistical assumptions, one can never gofrom almost 100 confidence to 100 confidence.This difference is crucial in many aspects of human thought. Of course,logical reasoning and mathematics stand out as prime examples. Long chainsof reasoning are only possible if the individual links are reliable. But it is alsocrucial in social reasoning. There is an essential difference between statisticaland evidencebased statements, say Michael is a popular name, and tautological, definitional and descriptive statements such as My name is Michael.While the first statement might be a subject of discussion, a model which canget confused about the second statement is clearly missing a defining aspect ofhuman thought, and will lose the confidence of its interlocutor. Perhaps epistemological status and tautological correctness need to be somehow representedin the model.It need not be designed in, but the model needs to be givenadditional signals beyond next word prediction to learn it.The third point on my list, reflection, does not seem to be much discussed,but to me seems just as important. In computer science, reflection is the capability of a system to work with its programs as a form of data 124, 138.This is naturally possible for a computer programmed in assembly language, inwhich instructions are encoded in integers. To some extent it is also possiblein Lisp, in which programs are encoded in a universal list data structure. Astype systems and other programming language refinements are introduced, reflection becomes more difficult to provide, but it is necessary for systemslevel30programming and makes various standard tasks easier to implement.Since an LLM operates on language, reflection for an LLM is the ability towork with its internal model in linguistic terms. This is related to ML interpretability, the ability to translate a model into understandable terms. In 7 wediscussed interpretability of LLMs in terms of circuits and computational models, implicitly leaving these for a human to interpret and understand. One canimagine an interpretation engine which given a model, automatically producesa more interpretable description, in terms of circuits, rules, or even a descriptionof the models functioning in natural language. Given such an interpretationengine, by applying it to an LLM and sending its output as an input to theLLM, we can implement a form of reflection.A basic human capability which corresponds to this process is the translationfrom procedural or other implicit forms of memory to linguistic, explicit memory. Very often, we learn by doing riding a bicycle, solving math problems,interacting socially.