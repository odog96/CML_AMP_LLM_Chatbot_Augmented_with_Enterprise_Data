Understanding how they work is both straightforward we explained it in 6 and at the same time an outstanding scientificchallenge. This is because the question how do they work has multiple meanings. On the one hand, LLMs are a relatively simple solution to the task ofpredicting the likely next word in a text. On the other hand, they also seem toperform many other tasks which require intelligence, such as solving the physicsword problem in Figure 1. While we do not have a strong understanding ofwhat a system which can perform these tasks must do, a vast body of work incognitive science and AI supports ones first naive intuition that such a systemmust be doing sophisticated analyses of language, must contain models of thereal world, and must be able to do fairly general logical reasoning. Before itwas demonstrated, the idea that all this could be learned as a byproduct of25word prediction would have seemed hopelessly optimistic, had anyone dared tosuggest it.Extraordinary claims should be greeted with skepticism. One must guardagainst the possibility that a successful ML system is actually picking up onsuperficial aspects or statistical regularities of the inputs, the clever Hanseffect. Addressing this is an important function of the benchmark evaluationsdiscussed in 4. Of course as LLMs get good at performing tasks of practicalvalue, the skeptical position becomes hard to maintain.Intelligence and language are incredibly complex and diverse. According toMinsky,40 this diversity is a defining feature of intelligence. The goal of understanding LLMs or any general AI will not be accomplished by understandingall of the content in their training data, the entire internet. Rather, the trickwe need to understand is how a single system can learn from this diverse corpusto perform a wide range of tasks. Theories of what is learnable are a centralpart of computer science 68. Although theoretical understanding has a longway to go to catch up with LLM capabilities, for simpler and better understoodtasks much is known.In these notes we mostly looked at this question through the lens of computerscience, and took as the gold standard for explaining how an LLM learns andperforms a task, a computational model expressed as an algorithm or a circuittogether with arguments that the trained LLM realizes this model. This point ofview has many more insights to offer, but before we discuss them let us considersome other points of view. In 7 we drew the analogy between detailed studyof transformer circuits and neuroscience what others can we consider?Another analogy is with cognitive psychology. LLMs are sufficiently humanlike to make this interesting, and there is a growing literature which applies testsand experimental protocols from psychology to LLMs, see for example 53 andthe many references there. When discussing this, we should keep in mind thevast differences between how humans and LLMs function. Human brains arenot believed to use the backpropagation learning algorithm, indeed it has beenargued that biological neural systems cannot use it 37.