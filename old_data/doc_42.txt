But it was long ago realizedthat once one goes beyond formal worlds such as chess and algebra to thecomplex and messy situations of real life, although one can postulate rules whichcapture many truths and can be used for reasoning, rules which are valid in allcases are very rare. Furthermore, the sheer number of rules required to covereven the likely possibilities is very large. These difficulties were addressed byimplementing probabilistic reasoning and by getting teams of humans to developthe requisite enormous rule sets, leading to the expert system approach whichwas applied for example to medical question answering. Cyc,8 an early andwell known expert system, is commercially available and has a database ofcommonly known facts with over 25 million assertions however this is dwarfedby knowledge bases such as Wikidata over one billion facts but which doIt is clear that any approach whichnot have a systematic reasoning engine.depends on careful human analysis of such large knowledge bases is impractical.Meanwhile, a very different connectionist approach to AI was being championed by other researchers. They drew their inspiration from hypotheses abouthow the brain works, from information theory and statistics, from physics andother natural sciences, and from applied mathematics and particularly optimization theory. These diverse points of view came together in the 1990s andled to a great deal of interdisciplinary work,9 of which the part most related toAI and which lies behind LLMs is called machine learning ML.The usual starting point in modern treatments of ML is to rephrase a taskas a statistical inference problem based on a large dataset. A canonical exampleis image recognition say, given an array of pixels light intensity values,estimate the probability that the image depicts a cat. Rather than design asystem to do this, one starts with a large dataset of images with labels cat andnoncat. One then designs a very general statistical model and trains it onthis dataset to predict the label given the image. This is supervised learning,one can also do selfsupervised learning in which the system predicts somepart of the data given other parts say, filling in part of an image. A thirdstandard ML paradigm is reinforcement learning, which applies to tasks whichinvolve choosing actions to fulfill a longer term goal. The classic example isgame playing, as in AlphaGo and AlphaZero.In any case, since the problem is formulated statistically, it is possible to7httpswww.sigsam.org8httpscyc.com9I first learned about this from 83, 101.5consider the training dataset one item at a time, and use it to incrementallyimprove the model. This is almost always done by formulating the task in termsof an objective function which measures the quality with which it is performed,for example the accuracy with which correct labels are assigned to images. Onethen takes a parameterized model and trains it by optimizing this function,evaluated on the training dataset, with respect to the model parameters. Forthe classic models of statistics this can be done analytically, as in a least squaresfit. For more general models one uses numerical methods, such as gradientdescent.