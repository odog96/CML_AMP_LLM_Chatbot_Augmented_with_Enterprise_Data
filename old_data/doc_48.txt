One can even do incontext metalearning ofmachine learning tasks such as linear regression see 4.Once it is established that the model can generalize from a few examples, afurther step towards human capabilities is to try zero examples, instead simplyexplaining the task in natural language. At this point it becomes difficult toclassify the tasks should we consider the task of writing code from a naturallanguage specification to be a form of translation, or an example of explainingthe task, or something else? The relation between the input text or prompt17A quantitative version of this claim is that performance for the emergent capabilityimproves rapidly at some threshold value of the word prediction loss. This claim is disputed,see 120, 133 for discussion.11Dataset Size tokensParameters nonembeddingCompute PFdays, nonembeddingTest Lossand the output has many surprising features. For example, a standard technique in LLM question answering which measurably improves performance is toprecede the question with a prompt such as I will answer this question helpfully and truthfully. Is this somehow biasing the network towards certain textsand away from others after all the internet corpus is hardly a reliable source oftruth ? Suppose we have a theory of how this works, how can we test it? Doesthe model know anything about the truth of statements? 25, 79As has been much reported, one of the major difficulties in using LLMsfor practical tasks is their propensity to invent facts especially citations andtheir limited ability to do logical reasoning, algebra and other symbolic tasks.A device for improving this, called chain of thought prompting, is to giveexamples say of question answer task for definiteness with some intermediatereasoning steps spelled out. This was used in the Minerva QA system 77which produced the example in Figure 1. Still the fraction of problems it solvedcorrectly is around 50 the later GPT4 is similar. Even for simpler questions,the reliability of GPT4 is more like 90. Much current research is devoted tothis problem of reliable reasoning, as we discuss in 8.4 Phenomenology of language modelsIn this section we discuss general claims, noninvasive experiments, and theoretical arguments which do not depend on microscopic details of the modelssuch as the trained weights.18 This includes evaluation of model capabiliities,qualitative observations and scaling laws.What can LLMs do? There is a huge body of work on this question, and anyattempt to review it would rapidly go out of date, but let us review the primarymethod for studying it. This is benchmarking, the development of standardizedsets of test items for which model accuracy can be evaluated in a reproducibleway. This is in principle straightforward if the input corresponds to a singlecorrect output, as in multiple choice question answering.19 If the answer is freeform text, one can use text comparison metrics such as the ROUGE score. Onecurrent standard for evaluating LLMs, BIGbench 127, combines 204 languagetasks at first publication they accept new tasks including translation, QA,puzzle solving, text classification and summarization, and tests of common sensereasoning. A leaderboard listing the current best LLMs is at 20.