A system which uses hierarchicalplanning for mathematical theorem proving was developed in 62.The next capability on my list, making and working with confidence judgments, has to do with the well known hallucination problem, that LLMs oftensimply invent statements, including untrue facts and imaginary citations. Whileadvantageous for a poetry generator, and bearable for a system which makessuggestions which an expert human user will verify, this is a huge obstacle tomany practical applications. Thus it is the subject of a great deal of research a few of this months papers are 43, 79, 81. Perhaps by the time you readthese words there will have already been major progress.Why are LLMs producing these hallucinations? One intuition is that theyare doing some sort of compression, analogous to JPEG image compression,which introduces errors 29. This point of view suggests that the problem willeventually be solved with larger models and perhaps better training protocolswhich focus on the more informative data items 126.A related intuition is that the problems follow from inability to properlygeneralize. This comes back to the point about world models a correctmodel, for example an internal encoding of place information, by definitioncorrectly treats the properties being modeled. Now suppose we grant that theLLM is solving some class of problems, not by constructing such a model, but byrulebased reasoning. In other words, the LLM somehow learns rules from thecorpus which it uses to make particular inferences which agree with the model.While such rules can cover any number of cases, there is no clear reason for sucha rule set to ever cover all cases.Another intuition is that the training data contains errors and this is reflectedin the results. Certainly the internet is not known for being a completely reliablesource of truth. This intuition also fits with the observation that adding code29computer programs to the training set improves natural language reasoning.Code is a good source of rules because almost all of it has been debugged, leadingto rules which are correct in their original context of course they might not becorrectly applied. It is a longstanding question whether internal representationsboth in AI and in humans are shared between different natural languages itwould be truly fascinating to know how much they are also shared with code.If this intuition is right, then LLMs reasoning capability might be improved bytraining on far more code and other content which is guaranteed to be correct.Such content could be generated synthetically as tautologies, or even better asformal verified mathematics as proposed in 129.Here is a different point of view the problem is not that the systems makethings up, after all creativity has value. Rather, it is that they do not providemuch indication about the confidence to place in a particular output, and donot have ways to adapt their reasoning to statements known at different levels ofconfidence. Much of our reasoning involves uncertain claims and claims whichturn out to be false, the point is to distinguish these from justified claims andkeep track of our confidence in each belief.