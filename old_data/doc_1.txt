3 2 0 2 l u J 1 1 L C . s c 1 v 2 8 7 5 0 . 7 0 3 2 v i X r a Large Language Models Michael R. Douglas CMSA, Harvard University Dept. of Physics, Stony Brook University mdouglascmsa.fas.harvard.edu July 2023 Abstract Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models LLMs such as OpenAIs GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architec ture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence. 1 1 Introduction At the end of November 2022, OpenAI released a system called ChatGPT which interacts with its users in natural language. It can answer questions, engage in dialogs, translate between languages and write computer code with a fluency and ability far exceeding all previous publically available systems. Although it falls well short of human abilities in many ways, still the large language model technology of which it is an example is widely considered to be a major advance in artificial intelligence.1 Few developments in science and technology entered the popular conscious ness as quickly as ChatGPT. There is no mystery about why. The ability to use language is a defining property of humanity, and for the first time a computer is doing this well enough to make a comparison with humans interesting. All of the hopes and fears which have developed around AI, robots and technology more generally are being brought into the discussion. In my opinion this is justified the speed of recent progress makes it urgent to better understand AI, to forecast its capabilities and limitations, and to make wise decisions about its development and use. With great opportunities will come great challenges, which will concern all of us. In these lecture notes we give an introduction to this subject for mathe maticians, physicists, and other scientists and readers who are mathematically knowledgeable but not necessarily expert in machine learning or artificial intel ligence. We begin with a very brief overview of AI in 2 to explain some ideas we consider to be essential context, the basic principles of the symbolic and connectionist approaches. In 3 we define statistical language models and relate the history of transformerbased LLMs up through GPT4. In 4 we discuss measures of what LLMs do and how well they do it. We then give a precise explanation of simpler language models in 5 and the transformer architecture in 6.