DiscoveringLatent Knowledge in Language Models Without Supervision, December2022. arXiv2212.03827 cs. URL httparxiv.orgabs2212.03827.3526 Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and KevinKnight. Recurrent Neural Networks as Weighted Language Recognizers,March 2018. arXiv1711.05408 cs. URL httparxiv.orgabs1711.05408, doi10.48550arXiv.1711.05408.27 Ethan A. Chi, John Hewitt, and Christopher D. Manning. Finding Universal Grammatical Relations in Multilingual BERT. arXiv2005.04511cs, May 2020. arXiv 2005.04511. URL httparxiv.orgabs2005.04511.28 David Chiang, Peter Cholak, and Anand Pillay. Tighter Bounds onthe Expressivity of Transformer Encoders, May 2023. arXiv2301.10743cs. URL httparxiv.orgabs2301.10743, doi10.48550arXiv.2301.10743.29 Ted Chiang. Chatgpt is a blurry jpeg of the web. The New Yorker,February 2023.30 Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. GeneratingLong Sequences with Sparse Transformers. April 2019. URL httpsarxiv.orgabs1904.10509v1.31 Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous,and Yann LeCun. The Loss Surfaces of Multilayer Networks, January2015. arXiv1412.0233 cs. URL httparxiv.orgabs1412.0233,doi10.48550arXiv.1412.0233.32 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. PaLM Scaling Language Modeling with Pathways. arXiv2204.02311 cs, April 2022.arXiv 2204.02311. URL httparxiv.orgabs2204.02311.33 Bilal Chughtai, Lawrence Chan, and Neel Nanda. A Toy Model of Universality Reverse Engineering How Networks Learn Group Operations,May 2023. arXiv2302.03025 cs, math. URL httparxiv.orgabs2302.03025, doi10.48550arXiv.2302.03025.34 Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. MathematicalFoundations for a Compositional Distributional Model of Meaning, March2010. arXiv1003.4394 cs, math. URL httparxiv.orgabs1003.4394, doi10.48550arXiv.1003.4394.35 Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on machine learning, pages 29902999.PMLR, 2016. arXiv1602.07576.36 Remi Coulom. Efficient selectivity and backup operators in montecarlotree search. In International conference on computers and games, pages7283. Springer, 2006.3637 Francis Crick. The recent excitement about neural networks. Nature,337129132, 1989.38 George V. Cybenko. Approximation by superpositions of a sigmoidalfunction. Mathematics of Control, Signals and Systems, 2303314, 1989.39 Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova.BERT Pretraining of Deep Bidirectional Transformers for Language Understanding. October 2018. arXiv 1810.04805. URL httpsarxiv.orgabs1810.04805v1.40 Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural NetworkApproximation. arXiv2012.14501 cs, math, December 2020. arXiv2012.14501. URL httparxiv.orgabs2012.14501.41 Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.Inductive Biases and Variable Creation in SelfAttention Mechanisms.arXiv2110.10090 cs, stat, October 2021. arXiv 2110.10090. URLhttparxiv.orgabs2110.10090.42 Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, TomHenighan, Shauna Kravec, Zac HatfieldDodds, Robert Lasenby, DawnDrain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, DarioAmodei, Martin Wattenberg, and Christopher Olah. Toy Models ofSuperposition, September 2022.arXiv2209.10652 cs. URL httparxiv.orgabs2209.10652, doi10.48550arXiv.2209.10652.43 Philip Feldman, James R. Foulds, and Shimei Pan. Trapping LLM Hallucinations Using Tagged Context Prompts, June 2023. arXiv2306.06085cs. URL httparxiv.orgabs2306.06085, doi10.48550arXiv.2306.06085.44 John Rupert Firth. Studies in linguistic analysis. WileyBlackwell, 1957.45 Jerry A Fodor. The modularity of mind. MIT press, 1983.46 Dan Friedman, Alexander Wettig, and Danqi Chen. Learning TransformerPrograms, June 2023. arXiv2306.01128 cs. URL httparxiv.orgabs2306.01128, doi10.48550arXiv.2306.01128.47 Artur dAvila Garcez and Luis C. Lamb. Neurosymbolic AI The 3rdWave, December 2020. arXiv2012.05876 cs. URL httparxiv.orgabs2012.05876.48 Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. WhatCan Transformers Learn InContext? A Case Study of Simple FunctionClasses, January 2023. arXiv2208.01066 cs. URL httparxiv.orgabs2208.01066, doi10.48550arXiv.2208.01066.3749 Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MITpress, 2016.50 Anirudh Goyal and Yoshua Bengio.Inductive Biases for Deep LearnarXiv2011.15091 cs,ing of HigherLevel Cognition, August 2022.stat.