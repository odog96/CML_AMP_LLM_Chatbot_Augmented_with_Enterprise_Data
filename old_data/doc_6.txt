Finally, despite the effectiveness of the trained model in performing a task, the large number of parameters often makes it very hard to understand how such a model works, and why a given input produces a particular output. This interpretability problem remains a key issue with deep learning models, and is the subject of much research 52. There are many other variations and hybrid approaches in the story. Another important one is the pattern recognition approach 19, 102. This is also based on statistical inference but like the symbolic approach it emphasizes the value of detailed understanding of the problem domain in designing the system. For example, one could handcode the initial layers of an image recognition network to detect lines or other significant features of the image. But unlike a purely symbolic approach, these features would be used as input to a general statistical or neural model. Another concept which illustrates the relation between the two approaches is probabilistic reasoning, the use of rules such as the chance of rain when it is cloudy is 50. One can state and use such rules in a symbolic approach see for example 69, the essential distinction with connectionism is not the use of probabilities but rather the representation of knowledge in terms of explicit and meaningful rules. As we suspect every reader has already heard, the symbolic approach was dominant from the early days until 2012, and along with many other suc cesses led to a superhuman chess player, but seemed inadequate for our other 6 In 2012 the two challenge tasks theorem proving and question answering. connectionist approach surpassed other approaches to computer vision 70, and ever since neural systems have gone from triumph to triumph. In 2016 the deep learning system AlphaZero surpassed the symbolic AI chess players and of course humans. Over the last few years, transformer models trained on a large corpus of natural language to predict each next word as it appears, have revolutionized the field of natural language processing. As we write this the state of the art GPT4 demonstrates truly remarkable performance at question answering, code generation and many other tasks 24. The simplest and arguably deepest explanation for this history is that it is a consequence of the exponential growth of computational power and training datasets, which continues to the present day. Given limited computing power and data, the ability of the symbolic and pattern recognition approaches to directly incorporate human understanding into a system is a significant advan tage. On the other hand, given sufficiently large computing power and data, this advantage is nullified and may even become disadvantageous, as the human effort required to code the system becomes the limiting resource. This point, that the most significant advances in AI and computation more generally have come from hardware improvements and replacing human engineering with data driven methods, is forcefully made by Sutton in his bitter lesson essay 128.