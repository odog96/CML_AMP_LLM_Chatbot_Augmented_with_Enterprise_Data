It is amazing that a model defined by a few short equations, trained to go through a text and simply predict each next word as it appears a task which seems only loosely related to any definition of intelligence can do tasks which obviously require intelligence, such as solving word problems like the one in Figure 1 below. At present nobody really understands how this works. Even the interpretation of what LLMs are doing is controversial, ranging from the belief that they are simply rearranging the sentences they were trained on, all the way to the belief that the LLMs are learning sophisticated models of the world and that simply scaling up the computations will produce artificial general intelligence. Any forecast for progress must take into account the current models shortcomings lack of long term memory and ability to plan, tendency to make up facts and hallucinate, unreliability in logical reasoning, etc.. Do these problems have technical solutions which will also look simple once we have them? Or are they more significant barriers? Most current work on LLMs takes an engineering and problem solving per 1A few of the many other milestones in LLM development are 22, 32, 39, 113, 114. 2 Question Assume that the variance of the first n natural numbers is 10, and the variance of the first m even natural numbers is 16. Compute m n. Model output Let 2 be the variance of the first m even natural numbers, and 2 be the variance of the first n natural numbers. Then, we have 2 n21 12 and 2 m21 . We solve the system of equations 3 n2 1 12 m2 1 3 10 16. This translates into the system n2 121 m2 48 1 49. Therefore, n 11 and m 7 , so n m 18 . Figure 1 A questionanswer pair solved by Minerva. From Lewkowycz et al 77, 1, Solving quantitative reasoning problems with language models, 2022. spective, but there are many interesting works which focus more on understand ing how LLMs work. One would think this should be far easier than understand ing how human brains work, as we have full knowledge of an LLMs microscopic workings and can do a wide variety of experiments on it.2 These efforts are in their early days, but in 7 we survey current approaches to understanding how LLMs do what they do. We conclude in 8 with more general discussion, some questions and potentially important developments to watch for. Before we start, let me say a little about my own background. I was trained as a theoretical physicist and most of my contributions to science are in string theory and its interface with mathematics, but I have followed AI fairly closely since the 80s and in detail since 2016. In addition I spent eight years in quan titative finance where I gained a good deal of handson experience with ma chine learning.