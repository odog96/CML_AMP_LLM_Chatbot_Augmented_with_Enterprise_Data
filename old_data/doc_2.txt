Any forecast for progress must take into account the currentmodels shortcomings lack of long term memory and ability to plan, tendencyto make up facts and hallucinate, unreliability in logical reasoning, etc.. Dothese problems have technical solutions which will also look simple once we havethem? Or are they more significant barriers?Most current work on LLMs takes an engineering and problem solving per1A few of the many other milestones in LLM development are 22, 32, 39, 113, 114.2Question Assume that the variance of the first n natural numbers is 10, andthe variance of the first m even natural numbers is 16. Compute m n.Model output Let 2 be the variance of the first m even natural numbers,and 2 be the variance of the first n natural numbers. Then, we have 2 n2112and 2 m21. We solve the system of equations3n2 112m2 13 10 16.This translates into the systemn2 121m2 48 1 49.Therefore, n 11 and m 7 , so n m 18 .Figure 1 A questionanswer pair solved by Minerva. From Lewkowycz et al77, 1, Solving quantitative reasoning problems with language models, 2022.spective, but there are many interesting works which focus more on understanding how LLMs work. One would think this should be far easier than understanding how human brains work, as we have full knowledge of an LLMs microscopicworkings and can do a wide variety of experiments on it.2 These efforts are intheir early days, but in 7 we survey current approaches to understanding howLLMs do what they do. We conclude in 8 with more general discussion, somequestions and potentially important developments to watch for.Before we start, let me say a little about my own background. I was trainedas a theoretical physicist and most of my contributions to science are in stringtheory and its interface with mathematics, but I have followed AI fairly closelysince the 80s and in detail since 2016. In addition I spent eight years in quantitative finance where I gained a good deal of handson experience with machine learning.I have given many lectures telling computer scientists aboutphysics and physicists about computational topics, and benefited from conversations with many people more than I can name here, but let me thankGerald Jay Sussman, David McAllester, Yann LeCun, Sanjeev Arora, SuryaGanguli, Jeremy Avigad, Vijay Balasubramanian, Dmitri Chklovskii, DavidDonoho, Steve Skiena, Christian Szegedy, Misha Tsodyks, Tony Wu and the2At least, this was the case before March 2023. Currently the weights and even the design parameters of GPT4, the most advanced LLM, are held in confidence by OpenAI asproprietary information.3many speakers in the CMSA New Technologies seminar series,3 and Josef Urbanand the AITP community.4 Thanks to David McAllester and Sergiy Verstyukfor comments on the first draft. These notes would not have been possible without their input and advice, and I hope their signal to noise ratio approachesthat of what they shared with me.2Symbolic and connectionist AIThe goal of artificial intelligence is to build computational systems which canperform tasks which require intelligence.