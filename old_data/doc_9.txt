To motivate andefficiently carry out such costly experiments, one needs some ability to predict inadvance how changes in model and dataset size will affect the training methodsfor example the optimal choice of learning rate and performance.An important advance in this direction was the observation of power lawscaling in language model performance 67. Figure 2 plots the test loss16 againstthe logarithms of the sizes and compute resources used, and these straight linescorrespond to a power law relation between size and perplexity. This scalingholds over many decades in model size and, while the exponents 0.076 to0.095 are rather small, this is a strong argument that larger models will havebetter performance. These ideas were also used to determine optimal modeldataset size tradeoff 58 and the scaling of hyperparameters 140. These resultswere a significant input into the decision to do this very expensive research.Year Model Number of Parameters Dataset size tokens2018110MGPT2018 BERT 340M1.5B2019 GPT2175B2020 GPT32022PaLM 540B2023 GPT41.4T ?1B3B10B500B780B?Table 1 Large Language Models MBT millionbilliontrillion. In manycases several model sizes were considered we quote the largest.14State of the art, in other words an improvement over all previously evaluated models.15httpscommoncrawl.org16This is Eq. 3 minus log perplexity evaluated on texts which were removed or held outof the training set, to get a measure of generalization ability.10Figure 2 Language modeling performance as a function of model size, datasetsize, and amount of compute used for training. From Kaplan et al, ScalingLaws for Neural Language Models, 2020 67.Now it should be realized that, while the measure being improved here isfairly objective, still there was no strong reason to think that improving it wouldlead to models with qualitatively new emergent capabilities. But it appearsthat this is what happened GPT3 and its finetuned cousins such as Codexwere able to do tasks, such as write computer code from a natural languagedescription, for which smaller models were almost worthless.17 We will discussmore of this progress shortly, and speculate a bit in the conclusions.One of the most interesting LLM phenomena is incontext learning, firstdiscussed in the original GPT3 paper 22. This refers to the ability of anLLM to carry out tasks different from its original objective without modifying its parameters, indeed without any need for additional training on the newtask fine tuning. Rather, after being given as input text a few examplesof inputoutput pairs, the LLM can be given another input and will generatea suitable output. Say the new task is question answering, then after a fewquestionanswer examples the LLM will answer the next question it is given.While intuition based on human abilities might find this unremarkable, it isactually quite unusual for an ML model and this is why the pretrainingfinetuning paradigm was the usual approach in previous work. Of course the training set already contains many examples of QA pairs. More striking are taskswhich are not much represented in the training set, such as finding anagramsor rearranging letters in words.