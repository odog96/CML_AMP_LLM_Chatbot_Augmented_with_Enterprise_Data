However unlabeled datasets are much larger and by 2015 or so there was a sense that self supervised learning was the next frontier 74, leading to more focus on masked word prediction. The history of transformer models starts with the 2017 proposal of Vaswani et al 132. Their model was designed for a translation task and was more com plicated than what we will explain in 6, but the essential idea to use attention and positional encoding to represent all the relations between the words in a text originated here and is fully present. The transformer architecture was taken up by many groups, and particularly influential 2018 works include BERT 39 and GPT 112. BERT was trained by masking arbitrary words in a sentence not just the next word, which allows the model to look backward and forward for context and leads to better results. However it is not straightforward to sample from such a model, and eventually the simpler next word prediction approach followed by GPT won out. Both of these models, and most work of this period, followed the paradigm of pretraining followed by fine tuning. The idea was to first train for word prediction on a very large corpus, to get a general purpose model. This would then be adapted to specific tasks such as question answering by fine tuning. This means doing a second pass of supervised learning on a much smaller labeled 12With the sign, L 0 and better models have smaller L. The term loss function is often used for an objective function with these properties. 13In CS this term generally refers to the large scale arrangement of components of a system. 9 dataset, replacing next word prediction by the objective function for the specific task. Say we are doing question answering, this could be the accuracy of the answers. This two step procedure was justified by the notion of transfer learning, meaning that the capabilities of the general purpose model transfer to related but different tasks. This approach led to SOTA14 results on many benchmarks and motivated much further work. Most importantly, a great deal of ingenuity and hard work was put into solving the engineering problems of training larger and larger models on larger and larger datasets. As for the data, a lot of text is available on the web, with one much used archive of this data provided by Common Crawl.15 Training can largely be done in parallel by dividing up this data, and the availability of large clusters of GPUenabled servers at industrial labs and through cloud computing meant that sufficient computing resources were available in principle. However, the overall cost of training scales as at least the product of model size and dataset size, and this was becoming expensive. While the precise cost figures for the GPT series are not public, it is estimated that a single training run of the largest GPT3 models cost tens of millions of dollars.