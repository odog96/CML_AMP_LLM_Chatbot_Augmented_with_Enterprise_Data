The trained model outputs only legal moves with very high accuracy, and the question is whether this is done using internal representations which reflect the state of the game board, say the presence of a given color tile in a given position. Following the probe paradigm, they obtain FFNs which, given intermediate activations, can predict whether a board position is occupied and by which color tile. Furthermore, after modifying the activations so that the FFNs output has flipped a tile color, the model predicts legal moves for the modified board state, confirming the identification. Neuroscientists can only dream of doing such targeted experiments. Numerous probe studies have been done on LLMs. One very basic ques tion is how they understand grammatical roles and relations such as subject, object and the like. This question can be sharpened to probing their internal representations for parse trees, a concept we review in the appendix. To get the targets for the probe, one can use a large dataset of sentences labeled with parse trees, the Penn Treebank 90. This was done for BERT in 27, 56, 88 by the following procedure denote the embedding in a fixed layer of word i as ui, then the model learns a projection P on this space, such that the distances di, j P ui uj in this inner product well approximate the distance be tween words i and j defined as the length of the shortest path connecting them in the parse tree. For BERT with d 1000 this worked well with a projection P of rank 50. Once one knows something about how information is represented by the models, one can go on to try to understand how the computations are done. One approach, also analogous to neuroscience, is to look for specific circuits which perform specific computations. An example of a circuit which appears in trained transformer models is the induction head 42, 107. This performs the following task given a sequence such as A B . . . A it predicts a repetition, in this example B. The matching between the tokens the two As in the example is done by attention. A number of works have proposed and studied such circuits, with various motivations and using various theoretical lenses interpretability and LLMs 106, incontext learning 107, 2, formal language theory 94, 28, computational complexity theory 41, 82, etc.. Reverse engineering a large network ab initio, i.e. with minimal assumptions about what it is doing, seems challenging, but maybe automated methods will be developed 33, 46. Another approach is to first develop a detailed computational model CM to perform a task without looking too much at the system under study, and then look for evidence for or against the hypothesis that the system under study uses it. This approach also has a long history in neuroscience 91 and ways to test such hypotheses have been much discussed.