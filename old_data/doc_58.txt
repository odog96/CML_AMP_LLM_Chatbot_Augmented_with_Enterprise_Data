We canalso follow approaches used in neuroscience, psychology, and cognitive science.An evident observation is that the paradigm of neuroscience careful studyof the microscopic workings of the system, following a reductionist philosophy is far more practical for ML models than it is for human brains, as the microscopic workings are fully explicit. This is not to say that it is easy, as we still facethe difficulty of extracting meaning from a system with billions of componentsand parameters. How could we do this for LLMs?One familiar starting point in neuroscience is to measure the activity ofneurons and try to correlate it with properties of the system inputs or outputs.The grandmother cell which fires when a subject sees his or her grandmotheris an extreme and controversial example. Better established are the placecells in the hippocampus which fire when an animal passes through a specificpart of its environment.Generally there is no reason why the representation should be so direct theremight be some neural code which maps stimuli onto specific combinationsor patterns of activity. The details of the neural code could even be differentbetween one individual and the next. Analogous concepts in LLMs are the mapsfrom input strings to intermediate results or activations. The first of theseis the embedding map Eq. 7. Considering each layer in succession, its outputssometimes called contextualized embeddings also define such a map. Thedetails of these maps depend on details of the model, the training dataset andthe choices made in the training procedure. Besides the hyperparameters, theseinclude the random initializations of the parameters, the order in which dataitems are considered in training and their grouping into batches. Even smalldifferences can be amplified by the nonlinear nature of the loss landscape.One way to deal with this indeterminacy is to look for structure in the mapswhich does not depend on these choices. The linear relations Eq. 9 between wordembeddings are a very elegant example, telling us and presumably the modelsomething about the meanings of the words they represent. Moving on to thelater layers, one can ask whether contextualized embeddings carry informationabout the grammatical role of a word, about other words it is associated tosuch as the referent of a pronoun, etc.. One can go on to ask whether anyof the many structures which one would think need to be represented tounderstand the real world, are visible in these embeddings.Many structures are too intricate to show up in linear relations. A moregeneral approach is to postulate a target for each training data item andtrain a probe model usually an FFN to predict it from the embeddings. Ifthis works, one can go on to modify the internal representation in a minimal waywhich changes the probe prediction, and check if this leads to the correspondingeffects on the output see 12 and references there.This procedure is simpler to explain in an example.