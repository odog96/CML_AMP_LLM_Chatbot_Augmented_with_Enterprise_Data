As an example tiles on an 8 8 board, and each move results in flipping some opponent pieces to the players color. The main point for us is that the function from moves to board state is easily computable yet very nonlocal and nonlinear. 38While this model shares the GPT architecture, it is not trained on any language data, just on Othello games. 23 of a research tactic which does not require opening the black box, one can consider illusions which fool the system in some way. The response to these will often depend on contingent and nonoptimal aspects of the model, so one can distinguish different models which solve the same task. A new class of predictions which becomes testable for LLMs is to look at performance as a function of model size depth number of parameters. A particular CM might require a certain model size or dataset properties in order to perform well. And of course, one can open the black box by assuming a particular CM, one can make predictions for what probe experiments should work. Simple tasks studied in this approach include modular addition 103 and linear regression 2, where several CMs gradient descent, ridge regression and exact least squares were compared. Turning to language processing, a CM for parsing by transformer LLMs was developed in Zhou et al 143. While this is too lengthy to explain in detail here, let us give the basic idea, starting from the PCFG framework discussed in the appendix. Rather than try to represent a parse tree in terms of nodes and edges, it is represented by giving each position i in the list of words a set of variables i,t,j, where t indexes a nonterminal a left hand side of a rule and j is another position. If i,t,j is turned on, this means that a rule with t on the l.h.s. was used to generate that part of the tree stretching from position i to position j. This can be generalized to let i,t,j be the probability that a rule is used. These variables and additional variables describing the rules used higher in the tree satisfy simple recursion relations the InsideOutside parsing algorithm 87. If the rules have at most two symbols on the r.h.s.,39 these recursion relations are quadratic in the variables. By encoding the variables as components of the embedding, they can be implemented using attention. Naively, this model predicts that embedding dimension p must be very large, of order the number of nonterminals times the length of a sentence. Since realistic grammars for English have many hundreds of nonterminals, this seems to contradict the good performance of transformers with p 1000. This problem is resolved by two observations, of which the first is that one can get fairly good parsing with many fewer 20 nonterminals. The second is compression, that embeddings and circuits which are simple and interpretable can be mapped into more randomlooking lower dimensional forms.