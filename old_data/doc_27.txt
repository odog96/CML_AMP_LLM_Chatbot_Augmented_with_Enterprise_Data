LLMs are sufficiently human like to make this interesting, and there is a growing literature which applies tests and experimental protocols from psychology to LLMs, see for example 53 and the many references there. When discussing this, we should keep in mind the vast differences between how humans and LLMs function. Human brains are not believed to use the backpropagation learning algorithm, indeed it has been argued that biological neural systems cannot use it 37. Perhaps related to this, brains are not feedforward networks but have many bidirectional connections. Whatever brains are doing, it works very well LLMs like other current deep learning systems need far more training data than humans. Furthermore, the LLMs we discussed do not interact with the world. Some argue that on philo sophical grounds, a model trained only on language prediction can never learn meaning 16. While I do not find this particular claim convincing, I agree that we should not assume that LLMs perform tasks the same way humans do. Still both similarities and differences are interesting can we make the analogies with cognitive psychology more precise? One analogy 17, 50, is with the well known concept of fast and slow think ing in behavioral psychology 66. To summarize, humans are postulated to have two modes of thought, system 1 which makes fast, intuitive judgments, 40What magical trick makes us intelligent? The trick is that there is no trick. The power of intelligence stems from our vast diversity, not from any single, perfect principle. 100 26 and system 2 which can focus attention and do calculations, logic, and plan ning. While system 2 is more general and less errorprone, using it requires conscious attention and effort. According to the analogy, LLMs implement sys tem 1 thinking, and are weak at system 2 thinking. In 84 it is argued that LLMs have formal linguistic competence but not functional competence. In plainer terms, they are solving problems by manip ulating language using rules, but they lack other mechanisms of human thought. While it may be surprising that a purely rulebased system could do all that LLMs can do, we do not have a good intuition about what rulebased systems with billions of rules can do. What are the other mechanisms? There is a longstanding hypothesis in cog nitive science, modularity of mind 45, according to which the human brain has many mental modules with different capabilities. These include a language module of the sort that Chomsky famously advocated and many others, includ ing one for geometric and physical reasoning, another for social reasoning and theory of mind, and perhaps others. Notably, formal logic and mathematical reasoning seem to call upon different brain regions from those which specialize in language 3, suggesting that these functions are performed by different men tal modules.