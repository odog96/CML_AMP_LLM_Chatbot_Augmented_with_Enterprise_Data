The entireinternet contains in order of magnitude 1012 words, and such a corpus willcontain only a vanishingly small fraction of the likely twenty word strings.27A more general principle which we can take from the Ngram model is the distributional hypothesis, which has been pithily summarized as you shall knowa word by the company it keeps. 44 In other words, by proper use of thestatistics of neighboring words, one can define quantities which capture properties and even the meanings of words. The simplest expression of this idea26This is roughly the time in which the gradient descent operates, see Eq. 16. In LLMs oneoften considers each data item only once in a training run, so it is related to but differentfrom dataset size.27Statistical estimates of perplexity are in the 100s, and the best current LLMs have perplexity 20.15is the cooccurrence matrix. Before explaining this, let us mention a detail ofpractical systems, which in place of words use tokens, meaningful componentsof words. A physics illustration is the word supersymmetrization. Even for anonphysicist reader encountering it for the first time, this word naturally breaksup into super, symmetry and ization, pieces which appear in many wordsand which are called tokens. And not only does this decomposition apply tomany words, it helps to understand their meaning. This process of replacingsingle words by strings of tokens tokenization is a first step in LLM processing, and henceforth when we say word we will mean word or token in thissense.Given a corpus, we define its N gram cooccurrence matrix MN to be theW W matrix whose w, w entry counts the number of N grams in thecorpus containing both words. This matrix defines a map from words to vectors W Rp7where the dimension p W, by taking a word to the corresponding columnof MN . Such a map is called a word embedding.Applying this map to each word independently, we can map a string of kwords in W k to a string of vectors, and this is the next step after tokenizationof LLM processing. One might worry that these are very high dimensionalvectors with many zero entries, which seems wasteful. A standard statisticalcure for this problem is to do principal component analysis PCA. In words,instead of columns of MN we use the columns of a p W matrix Z chosen suchthat Z tZ is the best rank p approximation to MN in the sense that it minimizestr Z tZ MN 2. One can do better, but this gives the right idea.Next, we feed this string of vectors into some machine learning model to getan output which we use to predict the next word.If we just want the mostlikely next word, a good way is to output a vector v Rp, and choose theword w which maximizes the inner product v w. We denote this relationshipas v w. More generally, the standard inverse map from a vector to aprobability distribution on words is the Boltzmann distribution on the innerproducts.