With each word position say with index i we will associate a state vector si which can depend on words up to wi and on the immediately previous state. Then, we let the map F determine both the next word and the next state as vi1, si1 F si, vi, vi1, vi2, . . . , vik1, 12 where the parenthesis notation on the left hand side means that the output vector of F is the concatenation of two direct summand output vectors. Mathematically, Eq. 12 is a discrete dynamical system. If we grant that F can be an arbitrary map, this is a very general class of systems. One way of characterizing its generality is through computational complexity theory, by asking what classes of computation it can perform. In 123 it was argued that the RNN is a universal computer, but this granted that the computation of F in Eq. 11 could use infinite precision numbers. Under realistic assumptions the right complexity class is a finite state machine, which can recognize regular languages 26, 134. We will say more from this point of view in 7. There are many variations on the RNN such as LSTMs 57, each with their own advantages, but we must move on. 6 Recipe for an LLM We are now ready to define the transformer model.30 It is simply another class of maps F from lists of vectors to a vector to be used in the prescription above. Indeed, it is a natural generalization of the FFN which is associated to permu tational symmetry. This is in direct analogy to the use of convolutional neural networks CNNs for image recognition, which are FFNs which are equivariant under the symmetry of translations in two dimensions which is natural for the set of images. A transformer is a composition of two types of functions layers taken in alternation, each mapping an input list of L vectors ui to an output list of L vectors vi. One of these is an FFN as previously discussed, but now applied to each embedding vector independently, so vi FF F N ui. 30Other reviews explaining these definitions include 109, 131. 18 The other layer type is called attention, and it is defined as follows ui vi W i cid88 ci,juj ci,j j1 exp ui B uj j1 exp ui B uj cid80i 13 14 where B is a learnable matrix whose elements are model parameters equiva lently, u B v is a bilinear form and W is a linear map also learnable. In words, an item vi in the output vector is a linear transformation of a weighted sum of the inputs uj with i j and can depend on any of them.31 The weights ci,j are given by a softmax or Boltzmann weight just as in Eq. 8. Thus there is a very general learnable way for each output to choose which of the input vectors are most useful as inputs.