Either way, a central question of statistics and machine learning isgeneralization, meaning the extent to which the model well describes data notin the training set but sampled from the same probability distribution. A wellknown principle which speaks to this question is Occams razor, that simplermodels will generalize better. This is often simplified to the rule that a modelshould have the minimal number of parameters needed to fit the dataset.Not all machine learning systems are deep learning 76 or connectionist 118. These terms generally refer to the use of neural networks with largenumbers of parameters which provide effective universal function approximators. While the idea is very old 117, before 2012 it was widely believed to beimpractical. One argument for this was the dull side of Occams razor models with so many parameters were destined to overfit and would not generalize.Evidently this is not the case, leading to concepts such as benign overfitting.10, 15 Another argument was that the objective functions for these modelsare highly nonconvex and optimization would get stuck at poor quality localminima. This can be a problem, but turns out to be solvable for reasons thatare partially understood 31, 49. Finally, despite the effectiveness of the trainedmodel in performing a task, the large number of parameters often makes it veryhard to understand how such a model works, and why a given input producesa particular output. This interpretability problem remains a key issue withdeep learning models, and is the subject of much research 52.There are many other variations and hybrid approaches in the story. Anotherimportant one is the pattern recognition approach 19, 102. This is also basedon statistical inference but like the symbolic approach it emphasizes the valueof detailed understanding of the problem domain in designing the system. Forexample, one could handcode the initial layers of an image recognition networkto detect lines or other significant features of the image. But unlike a purelysymbolic approach, these features would be used as input to a general statisticalor neural model.Another concept which illustrates the relation between the two approachesis probabilistic reasoning, the use of rules such as the chance of rain when it iscloudy is 50. One can state and use such rules in a symbolic approach seefor example 69, the essential distinction with connectionism is not the use ofprobabilities but rather the representation of knowledge in terms of explicit andmeaningful rules.As we suspect every reader has already heard, the symbolic approach wasdominant from the early days until 2012, and along with many other successes led to a superhuman chess player, but seemed inadequate for our other6In 2012 thetwo challenge tasks theorem proving and question answering.connectionist approach surpassed other approaches to computer vision 70, andever since neural systems have gone from triumph to triumph.In 2016 thedeep learning system AlphaZero surpassed the symbolic AI chess players andof course humans. Over the last few years, transformer models trained on alarge corpus of natural language to predict each next word as it appears, haverevolutionized the field of natural language processing.