These can be a combination of sines and31The restriction i j to previous or current inputs is done to get an autoregressive modelone can relax this for other purposes.32Explicitly, vi W1 max0, W0 ui b0 b1, where b0,1 are more learnable parameters.19cosines of various frequencies, such as 132e2i1, e2i cosposition100002idpos, sinposition100002idposi 1, . . . ,dpos2 15One could instead treat these vectors as learnable parameters. Still, the trigfunction basis for positions may be significant. It has been generalized to represent other graph structures by using eigenfunctions of the graph Laplacian aspositional embeddings.The invariance of the transformer model under permutation symmetry isreminiscent of the point we mentioned earlier, that translation symmetry motivates the CNN. However permutation symmetry is badly broken in language,even in the simplest formal languages,33 and it is not obvious why this shouldbe a useful property for the model to have. One might argue that although anyparticular language breaks permutation symmetry, it acts naturally on the ensemble of languages and thus should have a simple representation. For example,besides the usual infix arithmetic notation a b, one could instead use prefix a b or postfix a b . Translating between these notations is arguablyeasier for permutation invariant maps using position embeddings. An opposing view would be that permutation symmetry is just a secondary property ofthe simplest model using attention, and that the main point is to explain thevalue of attention. In addition to its ability to select similar items, it providesa simple way to take products of embedding vectors. In computational complexity terms, attention enlarges the class of circuits which can be simulated bya constant depth transformer 28, 41, 95, 96. Physics analogies of Eqs. 13,14,especially to the Hopfield model, may be important 59, 115.A major practical advantage of the transformer over the RNN and otherprevious architectures is that the computations in the attention mechanism canbe done in parallel, so given sufficiently many processors the time required doesnot increase with the window length L. This is by contrast with the RNN inwhich information propagates from one word to the next, so a window of lengthL requires time L to process. On the other hand the ability of each unit to payattention to every previous unit means that the total computation required bythe transformer scales as L2. This is the limiting factor for increasing L andthis is widely seen as a problem. There has been a lot of work to improve thisscaling, by removing some of the connections as in sparse attention 30, byintroducing multiscale structure, or in other ways.Let us summarize by listing the hyperparameters34 and their values for thelargest 175B GPT3 22. They are Embedding dimension p 12288 and hidden layer dimension ph 4p. Window length L 4096 or 8192. Depth D 96, counting both FFN Eq. 11 and attention Eq.